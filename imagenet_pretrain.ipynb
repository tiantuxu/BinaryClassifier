{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import tempfile\n",
    "import keras.optimizers\n",
    "import random\n",
    "import cv2\n",
    "from math import ceil\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "import ast\n",
    "import h5py\n",
    "from keras.utils import np_utils\n",
    "from itertools import tee\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noscope/DataUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def nth_elem(list, n):\n",
    "    return np.array([list[i] for i in xrange(0, len(list), n)])\n",
    "\n",
    "# def get_labels(csv_fname, limit=None, interval=1, start=0, labels=['person', 'bus', 'car']):\n",
    "#     df = pd.read_csv(csv_fname)\n",
    "#     df = df[df['frame'] >= start]\n",
    "#     df = df[df['frame'] < start + limit]\n",
    "#     df['frame'] -= start\n",
    "#     df = df[df['object_name'].isin(labels)]\n",
    "#     groups = df.set_index('frame')\n",
    "#     return groups\n",
    "\n",
    "def get_raw_counts(csv_fname, OBJECTS=['person'], limit=None, interval=1, start=0):\n",
    "    labels = get_labels(csv_fname, interval=interval, limit=limit, start=start)\n",
    "    counts = np.zeros( (len(labels), len(OBJECTS)), dtype='uint8' )\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, obj in enumerate(OBJECTS):\n",
    "            counts[i, j] = sum(map(lambda x: 1 if x['object_name'] == obj else 0, label))\n",
    "    return counts\n",
    "\n",
    "# FIXME: efficiency\n",
    "def get_counts(csv_fname, OBJECTS=['person'], limit=None, interval=1, start=0):\n",
    "    labels = get_labels(csv_fname, interval=interval, limit=limit, start=start)\n",
    "    counts = np.zeros( (len(labels), len(OBJECTS)), dtype='float' )\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, obj in enumerate(OBJECTS):\n",
    "            counts[i, j] = max([0] + \\\n",
    "                    map(lambda x: x['confidence'] if x['object_name'] == obj else 0, label))\n",
    "    return counts\n",
    "\n",
    "def get_differences(csv_fname, OBJECT, limit=None, interval=1, delay=1):\n",
    "    def sym_diff(first, second):\n",
    "        first_objs = set(x['object_name'] for x in first if x['object_name'] == OBJECT)\n",
    "        second_objs = set(x['object_name'] for x in second if x['object_name'] == OBJECT)\n",
    "        return len(first_objs.symmetric_difference(second_objs)) > 0\n",
    "\n",
    "    labels = get_labels(csv_fname, limit=limit, interval=interval, start=delay)\n",
    "    return np.array([1 if sym_diff(labels[i], labels[i-delay]) else 0 for i in xrange(delay, limit, interval)])\n",
    "\n",
    "def get_binary(csv_fname, OBJECTS=['person'], limit=None, start=0, WINDOW=30):\n",
    "    df = pd.read_csv(csv_fname)\n",
    "    df = df[df['object_name'].isin(OBJECTS)]\n",
    "    groups = df.set_index('frame')\n",
    "    counts = map(lambda i: i in groups.index, range(start, limit + start))\n",
    "    counts = np.array(counts)\n",
    "\n",
    "    smoothed_counts = np.convolve(np.ones(WINDOW), np.ravel(counts), mode='same') > WINDOW * 0.7\n",
    "    print np.sum(smoothed_counts != counts), np.sum(smoothed_counts)\n",
    "    smoothed_counts = smoothed_counts.reshape(len(counts), 1)\n",
    "    counts = smoothed_counts\n",
    "    return counts\n",
    "\n",
    "def smooth_binary(counts):\n",
    "    for i in xrange(1, len(counts) - 1):\n",
    "        if counts[i][0] > 0:\n",
    "            continue\n",
    "        if counts[i - 1][0] > 0 and counts[i + 1][0] > 0:\n",
    "            counts[i][0] = 1\n",
    "    return counts\n",
    "\n",
    "# Given X_train, X_test, center both by the X_train mean\n",
    "def center_data(X_train, X_test):\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    return X_train - mean, X_test - mean\n",
    "\n",
    "# Convert (frames, counts) into test, train\n",
    "def to_test_train(all_frames, all_counts,\n",
    "                  regression=False, center=True, dtype='float32', train_ratio=0.6):\n",
    "    assert len(all_frames) == len(all_counts), 'Frame length should equal counts length'\n",
    "\n",
    "    def split(arr):\n",
    "        # 250 -> 100, 50, 100\n",
    "        ind = int(len(arr) * train_ratio)\n",
    "        if ind > 100000:\n",
    "            ind = len(arr) - 100000\n",
    "        return arr[:ind], arr[ind:]\n",
    "\n",
    "    nb_classes = all_counts.max() + 1\n",
    "    X = all_frames\n",
    "    if regression:\n",
    "        Y = np.array(all_counts)\n",
    "    else:\n",
    "        Y = np_utils.to_categorical(all_counts, nb_classes)\n",
    "\n",
    "    if center:\n",
    "        X_train, X_test = center_data(*split(X))\n",
    "        X_train = X_train.astype(dtype)\n",
    "        X_test = X_test.astype(dtype)\n",
    "    else:\n",
    "        X_train, X_test = split(X)\n",
    "    Y_train, Y_test = split(Y)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def read_coco_dataset(coco_dir, object, resol=50):\n",
    "    def read_hdf5_file(coco_dir, object, resol, data_type):\n",
    "        fname = '%s/%s_%d_%s2014.h5' % (coco_dir, object, resol, data_type)\n",
    "        h5f = h5py.File(fname, 'r')\n",
    "        X = h5f['images'][:]\n",
    "        Y = h5f['labels'][:].astype('uint8')\n",
    "        # shuffle X and Y in unison\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(X)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(Y)\n",
    "        h5f.close()\n",
    "        return X, Y\n",
    "\n",
    "    X_train, Y_train = read_hdf5_file(coco_dir, object, resol, 'train')\n",
    "    X_val, Y_val = read_hdf5_file(coco_dir, object, resol, 'val')\n",
    "\n",
    "    assert np.max(Y_train) == np.max(Y_val)\n",
    "    nb_classes = np.max(Y_train) + 1\n",
    "    Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "    Y_val = np_utils.to_categorical(Y_val, nb_classes)\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "def get_data(csv_fname, video_fname, binary=False, num_frames=None,\n",
    "             regression=False, OBJECTS=['person'], resol=(50, 50),\n",
    "             center=True, dtype='float32', train_ratio=0.6):\n",
    "    def print_class_numbers(Y, nb_classes):\n",
    "        classes = Y.argmax(axis=-1)\n",
    "        for i in xrange(nb_classes):\n",
    "            print 'class %d: %d' % (i, np.sum(classes == i))\n",
    "\n",
    "    print '\\tParsing %s, extracting %s' % (csv_fname, str(OBJECTS))\n",
    "    if binary:\n",
    "        all_counts = get_binary(csv_fname, limit=num_frames, OBJECTS=OBJECTS)\n",
    "    else:\n",
    "        all_counts = get_counts(csv_fname, limit=num_frames, OBJECTS=OBJECTS)\n",
    "    print '\\tRetrieving all frames from %s' % video_fname\n",
    "    all_frames = get_all_frames(\n",
    "            len(all_counts), video_fname, scale=resol, dtype=dtype)\n",
    "    print '\\tSplitting data into training and test sets'\n",
    "    X_train, X_test, Y_train, Y_test = to_test_train(\n",
    "            all_frames, all_counts, regression=regression,\n",
    "            center=center, dtype=dtype, train_ratio=train_ratio)\n",
    "    if regression:\n",
    "        nb_classes = 1\n",
    "        print '(train) mean, std: %f, %f' % \\\n",
    "            (np.mean(Y_train), np.std(Y_train))\n",
    "        print '(test) mean, std: %f %f' % \\\n",
    "            (np.mean(Y_test), np.std(Y_test))\n",
    "    else:\n",
    "        nb_classes = all_counts.max() + 1\n",
    "        print '(train) positive examples: %d, total examples: %d' % \\\n",
    "            (np.count_nonzero(Y_train.argmax(axis=-1)),\n",
    "             len(Y_train))\n",
    "        print_class_numbers(Y_train, nb_classes)\n",
    "        print '(test) positive examples: %d, total examples: %d' % \\\n",
    "            (np.count_nonzero(Y_test.argmax(axis=-1)),\n",
    "             len(Y_test))\n",
    "        print_class_numbers(Y_test, nb_classes)\n",
    "\n",
    "    print 'shape of image: ' + str(all_frames[0].shape)\n",
    "    print 'number of classes: %d' % (nb_classes)\n",
    "\n",
    "    data = (X_train, Y_train, X_test, Y_test)\n",
    "    return data, nb_classes\n",
    "\n",
    "def get_class_weights(Y_train, class_weight_factor=1.0):\n",
    "    n_classes = max(Y_train) + 1\n",
    "    class_multiplier = np.array([1.0*class_weight_factor, 1.0/class_weight_factor])\n",
    "    class_weights = float(len(Y_train)) / (n_classes*np.bincount(Y_train)*class_multiplier)\n",
    "    return dict(zip(range(n_classes), class_weights))\n",
    "\n",
    "def output_csv(csv_fname, stats, headers):\n",
    "    df = pd.DataFrame(stats, columns=headers)\n",
    "    df.to_csv(csv_fname, index=False)\n",
    "\n",
    "def confidences_to_csv(csv_fname, confidences, OBJECT):\n",
    "    col_names = ['frame', 'labels']\n",
    "    labels = map(lambda conf: [{'confidence': conf, 'object_name': OBJECT}],\n",
    "                 confidences)\n",
    "    # because past fuccboi DK make yolo_standalone 1-indexed\n",
    "    frames = range(1, len(confidences) + 1)\n",
    "    output_csv(csv_fname, zip(frames, labels), col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noscope/VideoUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def VideoIterator(video_fname, scale=None, start=0, frameset=None):\n",
    "    cap = cv2.VideoCapture(video_fname)\n",
    "    # Seeks to the Nth frame. The next read is the N+1th frame\n",
    "    # In OpenCV 2.4, this is cv2.cv.CAP_PROP_POS_FRAMES (I think)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start - 1)\n",
    "    frame = 0\n",
    "    frame_ind = -1\n",
    "    if scale is not None:\n",
    "        try:\n",
    "            len(scale)\n",
    "            resol = scale\n",
    "            scale = None\n",
    "        except:\n",
    "            resol = None\n",
    "    while frame is not None:\n",
    "        frame_ind += 1\n",
    "#         _, frame = cap.read()\n",
    "        reteval = cap.grab()\n",
    "        if not reteval:\n",
    "            print 'Cannot grab next frame: ', frame_ind \n",
    "        if frameset is not None and frame_ind not in frameset:\n",
    "            continue\n",
    "        _, frame = cap.retrieve()\n",
    "#         print 'video iter: ', frame_ind\n",
    "        if scale is not None:\n",
    "            frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST)\n",
    "        elif resol is not None:\n",
    "            frame = cv2.resize(frame, resol, interpolation=cv2.INTER_NEAREST)\n",
    "        yield frame_ind, frame\n",
    "\n",
    "def VideoHistIterator(video_fname, scale=None, start=0):\n",
    "    from noscope.filters import ColorHistogram\n",
    "    vid_it = VideoIterator(video_fname, scale=scale, start=start)\n",
    "    frame = 0\n",
    "    while frame is not None:\n",
    "        frame_ind, frame = vid_it.next()\n",
    "        hist = ColorHistogram.compute_histogram(frame)\n",
    "        yield frame_ind, frame, hist\n",
    "\n",
    "def get_all_frames(num_frames, video_fname, scale=None, start=0, dtype='float32', frameset=None):\n",
    "    if video_fname[-4:] == '.bin':\n",
    "        RESOL = (50, 50) # FIXME\n",
    "        FRAME_SIZE = RESOL[0] * RESOL[0] * 3\n",
    "        f = open(video_fname, 'rb')\n",
    "        f.seek(start * FRAME_SIZE)\n",
    "        frames = np.fromfile(f, dtype='uint8', count=num_frames * FRAME_SIZE)\n",
    "        frames = frames.reshape((num_frames, RESOL[0], RESOL[1], 3))\n",
    "        return frames.astype('float32') / 255.\n",
    "\n",
    "    vid_it = VideoIterator(video_fname, scale=scale, start=start, frameset=frameset)\n",
    "\n",
    "    _, frame = vid_it.next()\n",
    "    frames = np.zeros( tuple([num_frames] + list(frame.shape)), dtype=dtype )\n",
    "    frames[0, :] = frame\n",
    "\n",
    "    for i in xrange(1, num_frames):\n",
    "        _, frame = vid_it.next()\n",
    "        frames[i, :] = frame\n",
    "\n",
    "    if dtype == 'float32':\n",
    "        frames /= 255.0\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noscope/Models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "computed_metrics = ['accuracy', 'mean_squared_error']\n",
    "\n",
    "# In case we want more callbacks\n",
    "def get_callbacks(model_fname, patience=2):\n",
    "    return [ModelCheckpoint(model_fname)]\n",
    "    return [EarlyStopping(monitor='loss',     patience=patience, min_delta=0.00001),\n",
    "            EarlyStopping(monitor='val_loss', patience=patience + 2, min_delta=0.0001),\n",
    "            ModelCheckpoint(model_fname, save_best_only=True)]\n",
    "\n",
    "def get_loss(regression):\n",
    "    if regression:\n",
    "        return 'mean_squared_error'\n",
    "    else:\n",
    "        return 'categorical_crossentropy'\n",
    "\n",
    "def get_optimizer(regression, nb_layers, lr_mult=1):\n",
    "    if regression:\n",
    "        return keras.optimizers.RMSprop(lr=0.001 / (1.5 * nb_layers) * lr_mult)\n",
    "    else:\n",
    "        return keras.optimizers.RMSprop(lr=0.001 * lr_mult)# / (5 * nb_layers))\n",
    "\n",
    "\n",
    "def generate_conv_net_base(\n",
    "        input_shape, nb_classes,\n",
    "        nb_dense=128, nb_filters=32, nb_layers=1, lr_mult=1,\n",
    "        kernel_size=(3, 3), stride=(1, 1),\n",
    "        regression=False):\n",
    "    assert nb_layers >= 0\n",
    "    assert nb_layers <= 3\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                            border_mode='same',\n",
    "                            input_shape=input_shape,\n",
    "                            subsample=stride,\n",
    "                            activation='relu'))\n",
    "    model.add(Convolution2D(nb_filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "    if nb_layers > 1:\n",
    "        model.add(Convolution2D(nb_filters * 2, 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(Convolution2D(nb_filters * 2, 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#         model.add(Dropout(0.25))\n",
    "\n",
    "    if nb_layers > 2:\n",
    "        model.add(Convolution2D(nb_filters * 4, 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(Convolution2D(nb_filters * 4, 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#         model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_dense, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    if not regression:\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "    loss = get_loss(regression)\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=get_optimizer(regression, nb_layers, lr_mult=lr_mult),\n",
    "                  metrics=computed_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_conv_net(input_shape, nb_classes,\n",
    "                      nb_dense=128, nb_filters=32, nb_layers=1, lr_mult=1,\n",
    "                      regression=False):\n",
    "    return generate_conv_net_base(\n",
    "            input_shape, nb_classes,\n",
    "            nb_dense=nb_dense, nb_filters=nb_filters, nb_layers=nb_layers, lr_mult=lr_mult,\n",
    "            regression=regression)\n",
    "\n",
    "# Data takes form (X_train, Y_train, X_test, Y_test)\n",
    "def run_model(model, data, batch_size=32, nb_epoch=1, patience=2,\n",
    "        validation_data=(None, None)):\n",
    "    X_train, Y_train, X_test, Y_test = data\n",
    "    print ('training samples: %d/%d, testing samples: %d/%d' % (\n",
    "            np.count_nonzero(Y_train.argmax(axis=-1)), X_train.shape[0],\n",
    "            np.count_nonzero(Y_test.argmax(axis=-1)), X_test.shape[0]))\n",
    "    temp_fname = tempfile.mkstemp(suffix='.hdf5', dir='/tmp/')[1]\n",
    "\n",
    "    # 50k should be a reasonable validation split\n",
    "    if validation_data[0] is None:\n",
    "        validation_split = 0.33333333\n",
    "        if len(Y_train) * validation_split > 50000.0:\n",
    "            validation_split = 50000.0 / float(len(Y_train))\n",
    "#         print validation_split\n",
    "\n",
    "        begin_train = time.time()\n",
    "        print 'shape: ', X_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n",
    "        model.fit(X_train, Y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  nb_epoch=nb_epoch,\n",
    "                  # validation_split=validation_split,\n",
    "                  # validation_data=(X_test, Y_test),\n",
    "                  shuffle=True,\n",
    "                  class_weight='auto',\n",
    "                  callbacks=get_callbacks(temp_fname, patience=patience))\n",
    "        train_time = time.time() - begin_train\n",
    "    else:\n",
    "        begin_train = time.time()\n",
    "        model.fit(X_train, Y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  nb_epoch=nb_epoch,\n",
    "                  validation_data=validation_data,\n",
    "                  shuffle=True,\n",
    "                  class_weight='auto',\n",
    "                  callbacks=get_callbacks(temp_fname, patience=patience)\n",
    "                 )\n",
    "        train_time = time.time() - begin_train\n",
    "\n",
    "    model.load_weights(temp_fname)\n",
    "    os.remove(temp_fname)\n",
    "\n",
    "    return train_time\n",
    "\n",
    "\n",
    "# def get_labels(model, X_test, batch_size=256, get_time=False):\n",
    "#     begin = time.time()\n",
    "#     ## Alternate way to compute the classes\n",
    "#     # proba = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "#     # predicted_labels = np_utils.probas_to_classes(proba)\n",
    "#     predicted_labels = model.predict_classes(X_test, batch_size=batch_size, verbose=0)\n",
    "#     end = time.time()\n",
    "#     if get_time:\n",
    "#         return predicted_labels, end - begin\n",
    "#     else:\n",
    "#         return predicted_labels\n",
    "\n",
    "\n",
    "def stats_from_proba(proba, Y_test):\n",
    "    # Binary and one output\n",
    "    if proba.shape[1] == 1:\n",
    "        proba = np.concatenate([1 - proba, proba], axis=1)\n",
    "    if len(Y_test.shape) == 1:\n",
    "        Y_test = np.transpose(np.array([1 - Y_test, Y_test]))\n",
    "    predicted_labels = proba.argmax(axis=-1)\n",
    "\n",
    "    true_labels = Y_test.argmax(axis=-1)\n",
    "    precision, recall, fbeta, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "            predicted_labels, true_labels)\n",
    "    accuracy = sklearn.metrics.accuracy_score(predicted_labels, true_labels)\n",
    "\n",
    "    num_penalties, thresh_low, thresh_high = \\\n",
    "        StatsUtils.yolo_oracle(Y_test[:, 1], proba[:, 1])\n",
    "    windowed_acc, windowed_supp = StatsUtils.windowed_accuracy(predicted_labels, Y_test)\n",
    "\n",
    "    metrics = {'precision': precision,\n",
    "               'recall': recall,\n",
    "               'fbeta': fbeta,\n",
    "               'support': support,\n",
    "               'accuracy': accuracy,\n",
    "               'penalities': num_penalties,\n",
    "               'windowed_accuracy': windowed_acc,\n",
    "               'windowed_support': windowed_supp}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model_regression(model, X_test, Y_test, batch_size=256):\n",
    "    begin = time.time()\n",
    "    raw_predictions = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    end = time.time()\n",
    "    mse = sklearn.metrics.mean_squared_error(Y_test, raw_predictions)\n",
    "\n",
    "    Y_classes = Y_test > 0.2 # FIXME\n",
    "    Y_classes = np.concatenate([1 - Y_classes, Y_classes], axis=1)\n",
    "\n",
    "    best = {'accuracy': 0}\n",
    "    for cutoff in np.arange(0.01, 0.75, 0.01):\n",
    "        predictions = raw_predictions > cutoff # FIXME\n",
    "        proba = np.concatenate([1 - predictions, predictions], axis=1)\n",
    "        metrics = stats_from_proba(proba, Y_classes)\n",
    "        metrics['cutoff'] = cutoff\n",
    "        print 'Cutoff: %f, metrics: %s' % (cutoff, str(metrics))\n",
    "        if metrics['accuracy'] > best['accuracy']:\n",
    "            best = metrics\n",
    "\n",
    "    metrics = best\n",
    "    metrics['mse'] = mse\n",
    "    metrics['test_time'] = end - begin\n",
    "    return metrics\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, batch_size=256):\n",
    "    predicted_labels, test_time = get_labels(model, X_test, batch_size, True)\n",
    "    true_labels = Y_test.argmax(axis=-1)\n",
    "\n",
    "    confusion = sklearn.metrics.confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Minor smoothing to prevent division by 0 errors\n",
    "    TN = float(confusion[0][0]) + 1\n",
    "    FN = float(confusion[1][0]) + 1\n",
    "    TP = float(confusion[1][1]) + 1\n",
    "    FP = float(confusion[0][1]) + 1\n",
    "    metrics = {'recall': TP / (TP + FN),\n",
    "               'specificity': TN / (FP + TN),\n",
    "               'precision': TP / (TP + FP),\n",
    "               'npv':  TN / (TN + FN),\n",
    "               'fpr': FP / (FP + TN),\n",
    "               'fdr': FP / (FP + TP),\n",
    "               'fnr': FN / (FN + TP),\n",
    "               'accuracy': (TP + TN) / (TP + FP + TN + FN),\n",
    "               'f1': (2 * TP) / (2 * TP + FP + FN),\n",
    "               'test_time': test_time}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def learn_and_eval(model, data, nb_epoch=2, batch_size=128,\n",
    "        validation_data=(None, None)):\n",
    "    X_train, Y_train, X_test, Y_test = data\n",
    "    train_time = run_model(model, data, nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size, validation_data=validation_data)\n",
    "    metrics = evaluate_model(model, X_test, Y_test, batch_size=batch_size)\n",
    "    return train_time, metrics\n",
    "\n",
    "\n",
    "# NOTE: assumes first two parameters are: (image_size, nb_classes)\n",
    "def try_params(model_gen, params, data,\n",
    "               output_dir, base_fname, model_name, OBJECT,\n",
    "               regression=False, nb_epoch=2, validation_data=(None, None)):\n",
    "    def metrics_names(metrics):\n",
    "        return sorted(metrics.keys())\n",
    "    def metrics_to_list(metrics):\n",
    "        return map(lambda key: metrics[key], metrics_names(metrics))\n",
    "\n",
    "    summary_csv_fname = os.path.join(\n",
    "            output_dir, base_fname + '_' + model_name + '_summary.csv')\n",
    "\n",
    "    X_train, Y_train, X_test, Y_test = data\n",
    "    to_write = []\n",
    "    for param in params:\n",
    "        param_base_fname = base_fname + '_' + model_name + '_' + '_'.join(map(str, param[2:]))\n",
    "        model_fname = os.path.join(\n",
    "                output_dir, param_base_fname + '.h5')\n",
    "        csv_fname = os.path.join(\n",
    "                output_dir, param_base_fname + '.csv')\n",
    "\n",
    "        # Make, train, and evaluate the model\n",
    "        model = model_gen(*param, regression=regression)\n",
    "        if regression:\n",
    "            train_time = run_model(model, data, nb_epoch=nb_epoch,\n",
    "                    validation_data=validation_data)\n",
    "            metrics = evaluate_model_regression(model, X_test, Y_test)\n",
    "        else:\n",
    "            train_time, metrics = learn_and_eval(model, data,\n",
    "                    validation_data=validation_data)\n",
    "\n",
    "        # Output predictions and save the model\n",
    "        # Redo some computation to save my sanity\n",
    "        conf1 = model.predict(X_train, batch_size=256, verbose=0)\n",
    "        conf2 = model.predict(X_test,  batch_size=256, verbose=0)\n",
    "        conf = np.concatenate([conf1, conf2])\n",
    "        if len(conf.shape) > 1:\n",
    "            assert len(conf.shape) == 2\n",
    "            assert conf.shape[1] <= 2\n",
    "            if conf.shape[1] == 2:\n",
    "                conf = conf[:, 1]\n",
    "            else:\n",
    "                conf = np.ravel(conf)\n",
    "        confidences_to_csv(csv_fname, conf, OBJECT)\n",
    "        model.save(model_fname)\n",
    "\n",
    "        to_write.append(list(param[2:]) + [train_time] + metrics_to_list(metrics))\n",
    "        print ('params: ', param)\n",
    "        print ('training time: ', train_time)\n",
    "        print ('metrics: ', metrics)\n",
    "        print\n",
    "    print to_write\n",
    "    # First two params don't need to be written out\n",
    "    param_column_names = map(lambda i: 'param' + str(i), xrange(len(params[0]) - 2))\n",
    "    column_names = param_column_names + ['train_time'] + metrics_names(metrics)\n",
    "    output_csv(summary_csv_fname, to_write, column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp/shuffled_small_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def to_test_train(avg_fname, all_frames, all_counts, train_ratio=0.6):\n",
    "#     assert len(all_frames) == len(all_counts), 'Frame length should equal counts length'\n",
    "\n",
    "#     nb_classes = all_counts.max() + 1\n",
    "#     X = all_frames\n",
    "\n",
    "#     mean = np.mean(X, axis=0)\n",
    "#     np.save(avg_fname, mean)\n",
    "\n",
    "#     N = 150000\n",
    "#     # N = 500000\n",
    "\n",
    "#     '''pos_inds = np.random.permutation(np.where(all_counts.ravel() == 0))\n",
    "#     pos_inds = pos_inds[0, 0 : N/2]\n",
    "#     neg_inds = np.random.permutation(np.where(all_counts.ravel() == 1))\n",
    "#     neg_inds = neg_inds[0, 0 : N/2]\n",
    "#     print pos_inds.shape\n",
    "#     print neg_inds.shape\n",
    "#     p = np.concatenate([pos_inds, neg_inds])\n",
    "#     np.random.shuffle(p)'''\n",
    "\n",
    "#     p = np.random.permutation(len(all_counts))\n",
    "#     p = p[0:N]\n",
    "\n",
    "#     Y = np_utils.to_categorical(all_counts, nb_classes)\n",
    "#     X, Y = X[p], Y[p]\n",
    "#     X -= mean\n",
    "\n",
    "#     def split(arr):\n",
    "#         # 250 -> 100, 50, 100\n",
    "#         ind = int(len(arr) * train_ratio)\n",
    "#         if ind > 50000:\n",
    "#             ind = len(arr) - 50000\n",
    "#         return arr[:ind], arr[ind:]\n",
    "\n",
    "#     X_train, X_test = split(X)\n",
    "#     Y_train, Y_test = split(Y)\n",
    "\n",
    "#     return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# def get_data(csv_fname, video_fname, avg_fname,\n",
    "#              num_frames=None, start_frame=0,\n",
    "#              OBJECTS=['person'], resol=(50, 50),\n",
    "#              center=True, dtype='float32', train_ratio=0.6):\n",
    "#     def print_class_numbers(Y, nb_classes):\n",
    "#         classes = Y.argmax(axis=-1)\n",
    "#         for i in xrange(nb_classes):\n",
    "#             print 'class %d: %d' % (i, np.sum(classes == i))\n",
    "\n",
    "#     print '\\tParsing %s, extracting %s' % (csv_fname, str(OBJECTS))\n",
    "#     all_counts = get_binary(csv_fname, limit=num_frames, OBJECTS=OBJECTS, start=start_frame)\n",
    "#     print '\\tRetrieving all frames from %s' % video_fname\n",
    "#     all_frames = get_all_frames(\n",
    "#             len(all_counts), video_fname, scale=resol, start=start_frame)\n",
    "#     print '\\tSplitting data into training and test sets'\n",
    "#     X_train, X_test, Y_train, Y_test = to_test_train(avg_fname, all_frames, all_counts)\n",
    "\n",
    "#     nb_classes = all_counts.max() + 1\n",
    "#     print '(train) positive examples: %d, total examples: %d' % \\\n",
    "#         (np.count_nonzero(Y_train.argmax(axis=-1)),\n",
    "#          len(Y_train))\n",
    "#     print_class_numbers(Y_train, nb_classes)\n",
    "#     print '(test) positive examples: %d, total examples: %d' % \\\n",
    "#         (np.count_nonzero(Y_test.argmax(axis=-1)),\n",
    "#          len(Y_test))\n",
    "#     print_class_numbers(Y_test, nb_classes)\n",
    "\n",
    "#     print 'shape of image: ' + str(all_frames[0].shape)\n",
    "#     print 'number of classes: %d' % (nb_classes)\n",
    "\n",
    "#     data = (X_train, Y_train, X_test, Y_test)\n",
    "#     return data, nb_classes\n",
    "\n",
    "# def get_csv_adaptively(csv_fname, num_frames, OBJ, interval=1, start=0):\n",
    "#     df = pd.read_csv(csv_fname)\n",
    "#     df = df[df['object_name'].isin([OBJ])]\n",
    "#     groups = df.set_index('frame')\n",
    "#     end = start + num_frames * interval\n",
    "#     res = {i: i in groups.index for i in range(start, end, interval)}\n",
    "#     return res\n",
    "\n",
    "# def get_data_adaptively(csv_fname, video_fname, avg_fname,\n",
    "#              num_frames=None, start_frame=0,\n",
    "#              OBJECTS=['person'], resol=(50, 50),\n",
    "#              center=True, dtype='float32', train_ratio=0.6, burst=0, interval=1, decoded_images=False):\n",
    "#     def print_class_numbers(Y, nb_classes):\n",
    "#         classes = Y.argmax(axis=-1)\n",
    "#         for i in xrange(nb_classes):\n",
    "#             print 'class %d: %d' % (i, np.sum(classes == i))\n",
    "#     print '\\tParsing %s, extracting %s' % (csv_fname, str(OBJECTS))\n",
    "#     all_counts = get_csv_adaptively(csv_fname, num_frames, OBJECTS[0],\n",
    "#                                     start=start_frame, burst=burst, interval=interval)\n",
    "#     print '\\tCutting frames %d %d' % (len(all_counts.keys()), num_frames)\n",
    "#     all_counts = sorted(all_counts.items(), key=lambda x: x[0])[:num_frames]\n",
    "#     all_indexs = [x[0] for x in all_counts]\n",
    "#     all_labels = np.array([x[1] for x in all_counts])\n",
    "#     print '\\tRetrieving all frames from %s' % video_fname\n",
    "#     if not decoded_images:\n",
    "#         all_frames = get_all_frames(\n",
    "#             num_frames, video_fname, scale=resol, start=start_frame, frameset=all_indexs)\n",
    "#     else:\n",
    "#         all_frames = get_frames_from_images(all_indexs, video_fname, resol=resol, start=start_frame)\n",
    "#     print '\\tSplitting data into training and test sets'\n",
    "#     X_train, X_test, Y_train, Y_test = to_test_train(avg_fname, all_frames, all_labels)\n",
    "\n",
    "#     nb_classes = all_labels.max() + 1\n",
    "#     print '(train) positive examples: %d, total examples: %d' % \\\n",
    "#         (np.count_nonzero(Y_train.argmax(axis=-1)),\n",
    "#          len(Y_train))\n",
    "#     print_class_numbers(Y_train, nb_classes)\n",
    "#     print '(test) positive examples: %d, total examples: %d' % \\\n",
    "#         (np.count_nonzero(Y_test.argmax(axis=-1)),\n",
    "#          len(Y_test))\n",
    "#     print_class_numbers(Y_test, nb_classes)\n",
    "\n",
    "#     print 'shape of image: ' + str(all_frames[0].shape)\n",
    "#     print 'number of classes: %d' % (nb_classes)\n",
    "\n",
    "#     data = (X_train, Y_train, X_test, Y_test)\n",
    "#     return data, nb_classes\n",
    "\n",
    "def to_test_train(all_frames, all_labels, avg_num=[], nb_classes=2):\n",
    "    assert len(all_frames) == len(all_labels), 'Frame length should equal counts length'\n",
    "    X = all_frames\n",
    "    if len(avg_num) == 0:\n",
    "        mean = np.mean(X, axis=0)\n",
    "    else:\n",
    "        mean = avg_num\n",
    "    Y = np_utils.to_categorical(all_labels, nb_classes)\n",
    "    X -= mean\n",
    "\n",
    "    return X, Y, mean\n",
    "\n",
    "def get_csv_samples(csv_fname, OBJ):\n",
    "    df = pd.read_csv(csv_fname)\n",
    "    df = df[df['object_name'].isin([OBJ])]\n",
    "    groups = df.set_index('frame')\n",
    "    return groups.index\n",
    "\n",
    "def get_labels(csv_res, frameset):\n",
    "    ret = [t in csv_res for t in frameset]\n",
    "    print ('reading label sum: %d, pos: %d' % (len(ret), sum(ret)))\n",
    "    return np_utils.to_categorical(ret, 2)\n",
    "\n",
    "def get_frames_from_images(frameset, video_fname, resol=(50, 50), start=0, dtype='float32'):\n",
    "    if frameset == None:\n",
    "        print 'ERROR: frameset is none!'\n",
    "        sys.exit()\n",
    "    print ('reading images... %d') % (len(frameset))\n",
    "    frames = np.zeros( tuple([len(frameset)] + list(resol) + [3]), dtype=dtype )\n",
    "    for i in range(len(frameset)):\n",
    "#         if i % 1000 == 0:\n",
    "#             sys.stdout.write(\"\\033[F\") # Cursor up one line\n",
    "#             clear_output(wait=True)\n",
    "#             display('Iteration '+str(i)+' Score: '+str(uniform(0, 1)))\n",
    "#             display('\\timages read %d/%d' % (i, len(frameset))\n",
    "        img_path = os.path.join(video_fname, str(frameset[i] + 1).zfill(7) + '.jpg')\n",
    "        frame = cv2.imread(img_path)\n",
    "        frame = cv2.resize(frame, resol, interpolation=cv2.INTER_NEAREST)\n",
    "        frames[i, :] = frame\n",
    "\n",
    "    if dtype == 'float32':\n",
    "        frames /= 255.0\n",
    "\n",
    "    return frames\n",
    "\n",
    "def get_train_test_data(csv_fname, video_fname, train_nums, OBJ, pos_train_ratio=None):\n",
    "    total_sample = range(0, 3000000)\n",
    "    total_sample_num = len(total_sample)\n",
    "    print ('reading csv file: %s %s' % (csv_fname, OBJ))\n",
    "    csv_ret = get_csv_samples(csv_fname, OBJ)\n",
    "    total_pos_sample = [t for t in total_sample if t in csv_ret]\n",
    "    total_pos_sample_num = len(total_pos_sample)\n",
    "    total_neg_sample = [t for t in total_sample if t not in csv_ret]\n",
    "    total_neg_sample_num = len(total_neg_sample)\n",
    "    print ('total sample: %d, positive sample: %d' % (total_sample_num, total_pos_sample_num))\n",
    "    if pos_train_ratio != None:\n",
    "        pos_train_nums = int(train_nums * pos_train_ratio)\n",
    "    else:\n",
    "        pos_train_nums = int(train_nums * (float(total_pos_sample_num) / total_sample_num))\n",
    "    print ('training sample: %d, positive training sample: %d' % (train_nums, pos_train_nums))\n",
    "#     print csv_ret\n",
    "    \n",
    "    # get training samples\n",
    "    pos_frame_ids = np.random.permutation(total_pos_sample)\n",
    "    pos_frame_ids = pos_frame_ids[0:pos_train_nums]\n",
    "    neg_frame_ids = np.random.permutation(total_neg_sample)\n",
    "    neg_frame_ids = neg_frame_ids[0:train_nums - pos_train_nums]\n",
    "    train_frame_ids = np.concatenate((pos_frame_ids, neg_frame_ids), axis=0)\n",
    "    train_frame_ids = sorted(train_frame_ids)\n",
    "    print ('reading training image files...%d' % (len(train_frame_ids)))\n",
    "    train_frames = get_frames_from_images(train_frame_ids, video_fname)\n",
    "    train_labels = np.array([i in csv_ret for i in train_frame_ids])\n",
    "    X_train, Y_train, mean = to_test_train(train_frames, train_labels)\n",
    "    \n",
    "    # get testing samples\n",
    "    test_frame_ids = range(0, total_sample_num, 30)\n",
    "    print ('reading testing image files...%d' % (len(test_frame_ids)))\n",
    "    test_frames = get_frames_from_images(test_frame_ids, video_fname)\n",
    "    test_labels = [i in total_pos_sample for i in test_frame_ids]\n",
    "#     print len(test_frames), len(test_labels)\n",
    "    X_test, Y_test, _ = to_test_train(test_frames, test_labels, avg_num=mean)\n",
    "    print ('testing sample: %d, positive testing sample: %d' %\n",
    "           (len(test_frames), np.count_nonzero(Y_test.argmax(axis=-1))))\n",
    "    return [X_train, Y_train, X_test, Y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Run Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_path = '/host/hybridvs_data/'\n",
    "# # video_name = 'coral-reef-long'\n",
    "# # OBJECT = 'person'\n",
    "# video_name = 'jackson-town-square'\n",
    "# OBJECT = 'bus'\n",
    "# csv_in = os.path.join(data_path, 'csv', video_name + '.csv')\n",
    "# video_in = os.path.join(data_path, 'videos/scaled_50X50', video_name + '.mp4')\n",
    "# output_dir = os.path.join(data_path, 'cnn-models')\n",
    "# avg_fname = video_name + '.npy'\n",
    "# start_frame = 0\n",
    "# resol = (50, 50)\n",
    "# objects = [OBJECT]\n",
    "# split_image_path = '/host/mengwei/frames_50X50_jackson-town-square/'\n",
    "# num_frames = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print 'Preparing data....'\n",
    "# gc.collect()\n",
    "# data = get_train_test_data(\n",
    "#         csv_in, split_image_path, num_frames, OBJECT, pos_train_ratio=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, batch_size=128,\n",
    "              regression=False, nb_epoch=2, validation_data=(None, None)):\n",
    "    train_time = run_model(model, data, nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size, validation_data=validation_data)\n",
    "    return train_time, model\n",
    "# def test_model_two_thre(model, data, batch_size=128, error_rate=1):\n",
    "#     X_train, Y_train, X_test, Y_test = data\n",
    "#     probs = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "#     pos_samples = np.count_nonzero(Y_test.argmax(axis=-1))\n",
    "#     error_tolerate = pos_samples * (error_rate / 100.0)\n",
    "#     total_test_num = probs.shape[0]\n",
    "#     print ('test num: %d, pos num: %d, tolerate: %d' % (total_test_num, pos_samples, error_tolerate))\n",
    "#     combined_res = np.column_stack((probs[:,1], Y_test[:,1]))\n",
    "#     combined_res = combined_res[combined_res[:,0].argsort()]\n",
    "#     print combined_res\n",
    "#     # get T_low\n",
    "#     _error = 0\n",
    "#     for i in range(total_test_num):\n",
    "#         _error += combined_res[i,1]\n",
    "#         if _error > error_tolerate:\n",
    "#             _error -= 1\n",
    "#             T_low = combined_res[i - 1, 0]\n",
    "#             pos_low = i - 1\n",
    "#             break\n",
    "#     # get T_high\n",
    "#     _error = 0\n",
    "#     for i in reversed(range(total_test_num)):\n",
    "#         _error += (1 - combined_res[i,1])\n",
    "#         if _error > error_tolerate:\n",
    "#             _error -= 1\n",
    "#             T_high = combined_res[i + 1, 0]\n",
    "#             pos_high = i + 1\n",
    "#             break\n",
    "#     filter_ratio = 1 - float(pos_high - pos_low) / total_test_num\n",
    "#     return filter_ratio, float(_error) / pos_samples, T_low, T_high\n",
    "# return list(threshold, recall, precision)\n",
    "def test_model_one_thre(model, data, batch_size=128):\n",
    "    _, _, X_test, Y_test = data\n",
    "    probs = model.predict(X_test, batch_size=128, verbose=0)\n",
    "    pos_samples = np.count_nonzero(Y_test.argmax(axis=-1))\n",
    "    total_test_num = probs.shape[0]\n",
    "    print ('test num: %d, pos num: %d' % (total_test_num, pos_samples))\n",
    "    combined_res = np.column_stack((probs[:,1], Y_test[:,1]))\n",
    "    combined_res = combined_res[combined_res[:,0].argsort()]\n",
    "    print combined_res\n",
    "    # recall = TP / (TP + FN)\n",
    "    # precision = TP / (TP + FP)\n",
    "    ret = []\n",
    "    FN = 0\n",
    "    for i in range(total_test_num - 1):\n",
    "        FN += combined_res[i,1]\n",
    "        TP = pos_samples - FN\n",
    "        FP = total_test_num - i - 1 - TP\n",
    "        recall = float(TP) / (TP + FN)\n",
    "        precision = float(TP) / (TP + FP)\n",
    "#         print FN, TP, FP\n",
    "        if len(ret) > 0 and ret[-1][1] == recall:\n",
    "            ret[-1][2] = max(precision, ret[-1][2])\n",
    "        else:\n",
    "            ret.append([combined_res[i,0], recall, precision])\n",
    "    return ret\n",
    "# def try_model(model, data, OBJECT):\n",
    "#     train_time, model = train_model(model, data, OBJECT)\n",
    "#     filter_ratio, error, T_low, T_high = test_model(model, data)\n",
    "#     print train_time, filter_ratio, error, T_low, T_high\n",
    "def try_model_one_thre(model, data, nb_epoch=2, batch_size=128):\n",
    "    train_time, model = train_model(model, data, nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "    ret = test_model_one_thre(model, data)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare the data for training and testing\n",
    "# data = [X_train, Y_train, X_test, Y_test]\n",
    "# X_train and X_test: np.array([N X 50 X 50 X 3]), using the function get_frames_from_images above\n",
    "# Y_train and Y_test: np.array([N X 2]), the second column is the chance of positive object,\n",
    "# for example: Y_train=np.array([[0, 1], [1, 0]]), the first sample contains the object, the second doesn't\n",
    "\n",
    "resol=(50, 50)\n",
    "#frameset=None\n",
    "# Data comes from ILSVRC2014, in /media/teddyxu/blue-2tb\n",
    "# n02924116 are bus images, can change as needed\n",
    "pos_names = os.listdir(\"/host/hybridvs_data/ILSVRC2014_DET_train/n02924116/\")\n",
    "frames = np.zeros(tuple([1000] + list(resol) + [3]), dtype='float32')\n",
    "\n",
    "# Read positive images from the disk\n",
    "for i in range(0, len(pos_names)):\n",
    "    #print len(pos_names)\n",
    "    pos_img_path = os.path.join('/host/hybridvs_data/ILSVRC2014_DET_train/n02924116/', pos_names[i])\n",
    "    #print pos_img_path\n",
    "    frame = cv2.imread(pos_img_path)\n",
    "    frame = cv2.resize(frame, resol, interpolation=cv2.INTER_NEAREST)\n",
    "    frames[i,:] = frame\n",
    "    \n",
    "#pos_frames /= 255.0\n",
    "\n",
    "# Read negative images from the disk\n",
    "neg_dir = os.listdir(\"/host/hybridvs_data/ILSVRC2014_DET_train/\")\n",
    "for i in range(0, 500):\n",
    "    # True labels\n",
    "    if neg_dir[i] == 'n02924116':\n",
    "        continue\n",
    "    # False Labels, Randomly picked from other directories\n",
    "    neg_names = os.listdir(os.path.join('/host/hybridvs_data/ILSVRC2014_DET_train/', neg_dir[i]))\n",
    "    neg_img_path = os.path.join('/host/hybridvs_data/ILSVRC2014_DET_train/', neg_dir[i], neg_names[0])\n",
    "    #print neg_img_path\n",
    "    frame = cv2.imread(neg_img_path)\n",
    "    frame = cv2.resize(frame, resol, interpolation=cv2.INTER_NEAREST)\n",
    "    frames[i + 500,:] = frame\n",
    "\n",
    "frames /= 255.0\n",
    "\n",
    "Y = []\n",
    "for i in range(0, 1000):\n",
    "    if i < 500:\n",
    "        Y.append([0,1])\n",
    "    else:\n",
    "        Y.append([1,0])\n",
    "\n",
    "#print Y\n",
    "\n",
    "list_seq = range(0, 1000)\n",
    "#print list_seq\n",
    "random.shuffle(list_seq)\n",
    "#print list_seq\n",
    "\n",
    "#print len(frames)\n",
    "\n",
    "X_train = np.random.rand(1000, 50, 50, 3)\n",
    "Y_train = np.random.randint(2, size=(1000, 2))\n",
    "X_test = np.random.rand(1000, 50, 50, 3)\n",
    "Y_test = np.random.randint(2, size=(1000, 2))\n",
    "\n",
    "# First 800 samples as training set\n",
    "for i in range(0, 800):\n",
    "    X_train[i] = frames[list_seq[i]]\n",
    "    Y_train[i] = Y[list_seq[i]]\n",
    "\n",
    "# First 200 samples as training set\n",
    "for i in range(0, 200):\n",
    "    X_test[i] = frames[list_seq[i + 800]]\n",
    "    Y_test[i] = Y[list_seq[i + 800]]\n",
    "    \n",
    "data = [X_train, Y_train, X_test, Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), activation=\"relu\", input_shape=(50, 50, 3...)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples: 450/1000, testing samples: 285/1000\n",
      "shape:  (1000, 50, 50, 3) (1000, 2) (1000, 50, 50, 3) (1000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:99: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 355us/step - loss: 1.6396 - acc: 0.5370 - mean_squared_error: 0.2916\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 145us/step - loss: 0.6995 - acc: 0.5850 - mean_squared_error: 0.2515\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 144us/step - loss: 0.6058 - acc: 0.7350 - mean_squared_error: 0.2093\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 143us/step - loss: 0.6371 - acc: 0.6900 - mean_squared_error: 0.2169\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 146us/step - loss: 0.5228 - acc: 0.7470 - mean_squared_error: 0.1762\n",
      "test num: 1000, pos num: 285\n",
      "[[0.25309771 0.        ]\n",
      " [0.25553218 0.        ]\n",
      " [0.29276204 0.        ]\n",
      " ...\n",
      " [0.87391955 1.        ]\n",
      " [0.88470554 1.        ]\n",
      " [0.88616997 1.        ]]\n",
      "precision 0.99 [0.46174463629722595, 0.9929824561403509, 0.29915433403805497]\n",
      "precision 0.95 [0.5027193427085876, 0.9719298245614035, 0.302071973827699]\n",
      "precision 0.9 [0.5027193427085876, 0.9719298245614035, 0.302071973827699]\n"
     ]
    }
   ],
   "source": [
    "# after data prepared, just run this cell\n",
    "K.clear_session()\n",
    "param = ((50, 50, 3), 2, 128, 32, 2)\n",
    "model = generate_conv_net_base(*param, regression=False)\n",
    "accuracy = try_model_one_thre(model, data, nb_epoch=5)\n",
    "RECALL_LIST = [0.99, 0.95, 0.90]\n",
    "for r in RECALL_LIST:\n",
    "    temp = [a for a in accuracy if a[1] >= r]\n",
    "    item = max(temp, key=lambda x: x[2])\n",
    "    print 'precision', r, item\n",
    "model.save('/host/hybridvs_data/models/imagenet_init_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
