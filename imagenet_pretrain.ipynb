{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import tempfile\n",
    "import keras.optimizers\n",
    "import random\n",
    "import cv2\n",
    "from math import ceil\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "import ast\n",
    "import h5py\n",
    "from keras.utils import np_utils\n",
    "from itertools import tee\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noscope/DataUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def nth_elem(list, n):\n",
    "    return np.array([list[i] for i in xrange(0, len(list), n)])\n",
    "\n",
    "# def get_labels(csv_fname, limit=None, interval=1, start=0, labels=['person', 'bus', 'car']):\n",
    "#     df = pd.read_csv(csv_fname)\n",
    "#     df = df[df['frame'] >= start]\n",
    "#     df = df[df['frame'] < start + limit]\n",
    "#     df['frame'] -= start\n",
    "#     df = df[df['object_name'].isin(labels)]\n",
    "#     groups = df.set_index('frame')\n",
    "#     return groups\n",
    "\n",
    "def get_raw_counts(csv_fname, OBJECTS=['person'], limit=None, interval=1, start=0):\n",
    "    labels = get_labels(csv_fname, interval=interval, limit=limit, start=start)\n",
    "    counts = np.zeros( (len(labels), len(OBJECTS)), dtype='uint8' )\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, obj in enumerate(OBJECTS):\n",
    "            counts[i, j] = sum(map(lambda x: 1 if x['object_name'] == obj else 0, label))\n",
    "    return counts\n",
    "\n",
    "# FIXME: efficiency\n",
    "def get_counts(csv_fname, OBJECTS=['person'], limit=None, interval=1, start=0):\n",
    "    labels = get_labels(csv_fname, interval=interval, limit=limit, start=start)\n",
    "    counts = np.zeros( (len(labels), len(OBJECTS)), dtype='float' )\n",
    "    for i, label in enumerate(labels):\n",
    "        for j, obj in enumerate(OBJECTS):\n",
    "            counts[i, j] = max([0] + \\\n",
    "                    map(lambda x: x['confidence'] if x['object_name'] == obj else 0, label))\n",
    "    return counts\n",
    "\n",
    "def get_differences(csv_fname, OBJECT, limit=None, interval=1, delay=1):\n",
    "    def sym_diff(first, second):\n",
    "        first_objs = set(x['object_name'] for x in first if x['object_name'] == OBJECT)\n",
    "        second_objs = set(x['object_name'] for x in second if x['object_name'] == OBJECT)\n",
    "        return len(first_objs.symmetric_difference(second_objs)) > 0\n",
    "\n",
    "    labels = get_labels(csv_fname, limit=limit, interval=interval, start=delay)\n",
    "    return np.array([1 if sym_diff(labels[i], labels[i-delay]) else 0 for i in xrange(delay, limit, interval)])\n",
    "\n",
    "def get_binary(csv_fname, OBJECTS=['person'], limit=None, start=0, WINDOW=30):\n",
    "    df = pd.read_csv(csv_fname)\n",
    "    df = df[df['object_name'].isin(OBJECTS)]\n",
    "    groups = df.set_index('frame')\n",
    "    counts = map(lambda i: i in groups.index, range(start, limit + start))\n",
    "    counts = np.array(counts)\n",
    "\n",
    "    smoothed_counts = np.convolve(np.ones(WINDOW), np.ravel(counts), mode='same') > WINDOW * 0.7\n",
    "    print np.sum(smoothed_counts != counts), np.sum(smoothed_counts)\n",
    "    smoothed_counts = smoothed_counts.reshape(len(counts), 1)\n",
    "    counts = smoothed_counts\n",
    "    return counts\n",
    "\n",
    "def smooth_binary(counts):\n",
    "    for i in xrange(1, len(counts) - 1):\n",
    "        if counts[i][0] > 0:\n",
    "            continue\n",
    "        if counts[i - 1][0] > 0 and counts[i + 1][0] > 0:\n",
    "            counts[i][0] = 1\n",
    "    return counts\n",
    "\n",
    "# Given X_train, X_test, center both by the X_train mean\n",
    "def center_data(X_train, X_test):\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    return X_train - mean, X_test - mean\n",
    "\n",
    "# Convert (frames, counts) into test, train\n",
    "def to_test_train(all_frames, all_counts,\n",
    "                  regression=False, center=True, dtype='float32', train_ratio=0.6):\n",
    "    assert len(all_frames) == len(all_counts), 'Frame length should equal counts length'\n",
    "\n",
    "    def split(arr):\n",
    "        # 250 -> 100, 50, 100\n",
    "        ind = int(len(arr) * train_ratio)\n",
    "        if ind > 100000:\n",
    "            ind = len(arr) - 100000\n",
    "        return arr[:ind], arr[ind:]\n",
    "\n",
    "    nb_classes = all_counts.max() + 1\n",
    "    X = all_frames\n",
    "    if regression:\n",
    "        Y = np.array(all_counts)\n",
    "    else:\n",
    "        Y = np_utils.to_categorical(all_counts, nb_classes)\n",
    "\n",
    "    if center:\n",
    "        X_train, X_test = center_data(*split(X))\n",
    "        X_train = X_train.astype(dtype)\n",
    "        X_test = X_test.astype(dtype)\n",
    "    else:\n",
    "        X_train, X_test = split(X)\n",
    "    Y_train, Y_test = split(Y)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def read_coco_dataset(coco_dir, object, resol=50):\n",
    "    def read_hdf5_file(coco_dir, object, resol, data_type):\n",
    "        fname = '%s/%s_%d_%s2014.h5' % (coco_dir, object, resol, data_type)\n",
    "        h5f = h5py.File(fname, 'r')\n",
    "        X = h5f['images'][:]\n",
    "        Y = h5f['labels'][:].astype('uint8')\n",
    "        # shuffle X and Y in unison\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(X)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(Y)\n",
    "        h5f.close()\n",
    "        return X, Y\n",
    "\n",
    "    X_train, Y_train = read_hdf5_file(coco_dir, object, resol, 'train')\n",
    "    X_val, Y_val = read_hdf5_file(coco_dir, object, resol, 'val')\n",
    "\n",
    "    assert np.max(Y_train) == np.max(Y_val)\n",
    "    nb_classes = np.max(Y_train) + 1\n",
    "    Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "    Y_val = np_utils.to_categorical(Y_val, nb_classes)\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "def get_data(csv_fname, video_fname, binary=False, num_frames=None,\n",
    "             regression=False, OBJECTS=['person'], resol=(50, 50),\n",
    "             center=True, dtype='float32', train_ratio=0.6):\n",
    "    def print_class_numbers(Y, nb_classes):\n",
    "        classes = Y.argmax(axis=-1)\n",
    "        for i in xrange(nb_classes):\n",
    "            print 'class %d: %d' % (i, np.sum(classes == i))\n",
    "\n",
    "    print '\\tParsing %s, extracting %s' % (csv_fname, str(OBJECTS))\n",
    "    if binary:\n",
    "        all_counts = get_binary(csv_fname, limit=num_frames, OBJECTS=OBJECTS)\n",
    "    else:\n",
    "        all_counts = get_counts(csv_fname, limit=num_frames, OBJECTS=OBJECTS)\n",
    "    print '\\tRetrieving all frames from %s' % video_fname\n",
    "    all_frames = get_all_frames(\n",
    "            len(all_counts), video_fname, scale=resol, dtype=dtype)\n",
    "    print '\\tSplitting data into training and test sets'\n",
    "    X_train, X_test, Y_train, Y_test = to_test_train(\n",
    "            all_frames, all_counts, regression=regression,\n",
    "            center=center, dtype=dtype, train_ratio=train_ratio)\n",
    "    if regression:\n",
    "        nb_classes = 1\n",
    "        print '(train) mean, std: %f, %f' % \\\n",
    "            (np.mean(Y_train), np.std(Y_train))\n",
    "        print '(test) mean, std: %f %f' % \\\n",
    "            (np.mean(Y_test), np.std(Y_test))\n",
    "    else:\n",
    "        nb_classes = all_counts.max() + 1\n",
    "        print '(train) positive examples: %d, total examples: %d' % \\\n",
    "            (np.count_nonzero(Y_train.argmax(axis=-1)),\n",
    "             len(Y_train))\n",
    "        print_class_numbers(Y_train, nb_classes)\n",
    "        print '(test) positive examples: %d, total examples: %d' % \\\n",
    "            (np.count_nonzero(Y_test.argmax(axis=-1)),\n",
    "             len(Y_test))\n",
    "        print_class_numbers(Y_test, nb_classes)\n",
    "\n",
    "    print 'shape of image: ' + str(all_frames[0].shape)\n",
    "    print 'number of classes: %d' % (nb_classes)\n",
    "\n",
    "    data = (X_train, Y_train, X_test, Y_test)\n",
    "    return data, nb_classes\n",
    "\n",
    "def get_class_weights(Y_train, class_weight_factor=1.0):\n",
    "    n_classes = max(Y_train) + 1\n",
    "    class_multiplier = np.array([1.0*class_weight_factor, 1.0/class_weight_factor])\n",
    "    class_weights = float(len(Y_train)) / (n_classes*np.bincount(Y_train)*class_multiplier)\n",
    "    return dict(zip(range(n_classes), class_weights))\n",
    "\n",
    "def output_csv(csv_fname, stats, headers):\n",
    "    df = pd.DataFrame(stats, columns=headers)\n",
    "    df.to_csv(csv_fname, index=False)\n",
    "\n",
    "def confidences_to_csv(csv_fname, confidences, OBJECT):\n",
    "    col_names = ['frame', 'labels']\n",
    "    labels = map(lambda conf: [{'confidence': conf, 'object_name': OBJECT}],\n",
    "                 confidences)\n",
    "    # because past fuccboi DK make yolo_standalone 1-indexed\n",
    "    frames = range(1, len(confidences) + 1)\n",
    "    output_csv(csv_fname, zip(frames, labels), col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noscope/VideoUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def VideoIterator(video_fname, scale=None, start=0, frameset=None):\n",
    "    cap = cv2.VideoCapture(video_fname)\n",
    "    # Seeks to the Nth frame. The next read is the N+1th frame\n",
    "    # In OpenCV 2.4, this is cv2.cv.CAP_PROP_POS_FRAMES (I think)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start - 1)\n",
    "    frame = 0\n",
    "    frame_ind = -1\n",
    "    if scale is not None:\n",
    "        try:\n",
    "            len(scale)\n",
    "            resol = scale\n",
    "            scale = None\n",
    "        except:\n",
    "            resol = None\n",
    "    while frame is not None:\n",
    "        frame_ind += 1\n",
    "#         _, frame = cap.read()\n",
    "        reteval = cap.grab()\n",
    "        if not reteval:\n",
    "            print 'Cannot grab next frame: ', frame_ind \n",
    "        if frameset is not None and frame_ind not in frameset:\n",
    "            continue\n",
    "        _, frame = cap.retrieve()\n",
    "#         print 'video iter: ', frame_ind\n",
    "        if scale is not None:\n",
    "            frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST)\n",
    "        elif resol is not None:\n",
    "            frame = cv2.resize(frame, resol, interpolation=cv2.INTER_NEAREST)\n",
    "        yield frame_ind, frame\n",
    "\n",
    "def VideoHistIterator(video_fname, scale=None, start=0):\n",
    "    from noscope.filters import ColorHistogram\n",
    "    vid_it = VideoIterator(video_fname, scale=scale, start=start)\n",
    "    frame = 0\n",
    "    while frame is not None:\n",
    "        frame_ind, frame = vid_it.next()\n",
    "        hist = ColorHistogram.compute_histogram(frame)\n",
    "        yield frame_ind, frame, hist\n",
    "\n",
    "def get_all_frames(num_frames, video_fname, scale=None, start=0, dtype='float32', frameset=None):\n",
    "    if video_fname[-4:] == '.bin':\n",
    "        RESOL = (50, 50) # FIXME\n",
    "        FRAME_SIZE = RESOL[0] * RESOL[0] * 3\n",
    "        f = open(video_fname, 'rb')\n",
    "        f.seek(start * FRAME_SIZE)\n",
    "        frames = np.fromfile(f, dtype='uint8', count=num_frames * FRAME_SIZE)\n",
    "        frames = frames.reshape((num_frames, RESOL[0], RESOL[1], 3))\n",
    "        return frames.astype('float32') / 255.\n",
    "\n",
    "    vid_it = VideoIterator(video_fname, scale=scale, start=start, frameset=frameset)\n",
    "\n",
    "    _, frame = vid_it.next()\n",
    "    frames = np.zeros( tuple([num_frames] + list(frame.shape)), dtype=dtype )\n",
    "    frames[0, :] = frame\n",
    "\n",
    "    for i in xrange(1, num_frames):\n",
    "        _, frame = vid_it.next()\n",
    "        frames[i, :] = frame\n",
    "\n",
    "    if dtype == 'float32':\n",
    "        frames /= 255.0\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noscope/Models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "computed_metrics = ['accuracy', 'mean_squared_error']\n",
    "\n",
    "# In case we want more callbacks\n",
    "def get_callbacks(model_fname, patience=2):\n",
    "    return [ModelCheckpoint(model_fname)]\n",
    "    return [EarlyStopping(monitor='loss',     patience=patience, min_delta=0.00001),\n",
    "            EarlyStopping(monitor='val_loss', patience=patience + 2, min_delta=0.0001),\n",
    "            ModelCheckpoint(model_fname, save_best_only=True)]\n",
    "\n",
    "def get_loss(regression):\n",
    "    if regression:\n",
    "        return 'mean_squared_error'\n",
    "    else:\n",
    "        return 'categorical_crossentropy'\n",
    "\n",
    "def get_optimizer(regression, nb_layers, lr_mult=1):\n",
    "    if regression:\n",
    "        return keras.optimizers.RMSprop(lr=0.001 / (1.5 * nb_layers) * lr_mult)\n",
    "    else:\n",
    "        return keras.optimizers.RMSprop(lr=0.001 * lr_mult)# / (5 * nb_layers))\n",
    "\n",
    "\n",
    "def generate_conv_net_base(\n",
    "        input_shape, nb_classes,\n",
    "        nb_dense=128, nb_filters=32, nb_layers=1, lr_mult=1,\n",
    "        kernel_size=(3, 3), stride=(1, 1),\n",
    "        regression=False):\n",
    "    assert nb_layers >= 0\n",
    "    assert nb_layers <= 3\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                            border_mode='same',\n",
    "                            input_shape=input_shape,\n",
    "                            subsample=stride,\n",
    "                            activation='relu'))\n",
    "    model.add(Convolution2D(nb_filters, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "    if nb_layers > 1:\n",
    "        model.add(Convolution2D(nb_filters * 2, 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(Convolution2D(nb_filters * 2, 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#         model.add(Dropout(0.25))\n",
    "\n",
    "    if nb_layers > 2:\n",
    "        model.add(Convolution2D(nb_filters * 4, 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(Convolution2D(nb_filters * 4, 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#         model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_dense, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    if not regression:\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "    loss = get_loss(regression)\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=get_optimizer(regression, nb_layers, lr_mult=lr_mult),\n",
    "                  metrics=computed_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_conv_net(input_shape, nb_classes,\n",
    "                      nb_dense=128, nb_filters=32, nb_layers=1, lr_mult=1,\n",
    "                      regression=False):\n",
    "    return generate_conv_net_base(\n",
    "            input_shape, nb_classes,\n",
    "            nb_dense=nb_dense, nb_filters=nb_filters, nb_layers=nb_layers, lr_mult=lr_mult,\n",
    "            regression=regression)\n",
    "\n",
    "# Data takes form (X_train, Y_train, X_test, Y_test)\n",
    "def run_model(model, data, batch_size=32, nb_epoch=1, patience=2,\n",
    "        validation_data=(None, None)):\n",
    "    X_train, Y_train, X_test, Y_test = data\n",
    "    print ('training samples: %d/%d, testing samples: %d/%d' % (\n",
    "            np.count_nonzero(Y_train.argmax(axis=-1)), X_train.shape[0],\n",
    "            np.count_nonzero(Y_test.argmax(axis=-1)), X_test.shape[0]))\n",
    "    temp_fname = tempfile.mkstemp(suffix='.hdf5', dir='/tmp/')[1]\n",
    "\n",
    "    # 50k should be a reasonable validation split\n",
    "    if validation_data[0] is None:\n",
    "        validation_split = 0.33333333\n",
    "        if len(Y_train) * validation_split > 50000.0:\n",
    "            validation_split = 50000.0 / float(len(Y_train))\n",
    "#         print validation_split\n",
    "\n",
    "        begin_train = time.time()\n",
    "        print 'shape: ', X_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n",
    "        model.fit(X_train, Y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  nb_epoch=nb_epoch,\n",
    "                  # validation_split=validation_split,\n",
    "                  # validation_data=(X_test, Y_test),\n",
    "                  shuffle=True,\n",
    "                  class_weight='auto',\n",
    "                  callbacks=get_callbacks(temp_fname, patience=patience))\n",
    "        train_time = time.time() - begin_train\n",
    "    else:\n",
    "        begin_train = time.time()\n",
    "        model.fit(X_train, Y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  nb_epoch=nb_epoch,\n",
    "                  validation_data=validation_data,\n",
    "                  shuffle=True,\n",
    "                  class_weight='auto',\n",
    "                  callbacks=get_callbacks(temp_fname, patience=patience)\n",
    "                 )\n",
    "        train_time = time.time() - begin_train\n",
    "\n",
    "    model.load_weights(temp_fname)\n",
    "    os.remove(temp_fname)\n",
    "\n",
    "    return train_time\n",
    "\n",
    "\n",
    "# def get_labels(model, X_test, batch_size=256, get_time=False):\n",
    "#     begin = time.time()\n",
    "#     ## Alternate way to compute the classes\n",
    "#     # proba = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "#     # predicted_labels = np_utils.probas_to_classes(proba)\n",
    "#     predicted_labels = model.predict_classes(X_test, batch_size=batch_size, verbose=0)\n",
    "#     end = time.time()\n",
    "#     if get_time:\n",
    "#         return predicted_labels, end - begin\n",
    "#     else:\n",
    "#         return predicted_labels\n",
    "\n",
    "\n",
    "def stats_from_proba(proba, Y_test):\n",
    "    # Binary and one output\n",
    "    if proba.shape[1] == 1:\n",
    "        proba = np.concatenate([1 - proba, proba], axis=1)\n",
    "    if len(Y_test.shape) == 1:\n",
    "        Y_test = np.transpose(np.array([1 - Y_test, Y_test]))\n",
    "    predicted_labels = proba.argmax(axis=-1)\n",
    "\n",
    "    true_labels = Y_test.argmax(axis=-1)\n",
    "    precision, recall, fbeta, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "            predicted_labels, true_labels)\n",
    "    accuracy = sklearn.metrics.accuracy_score(predicted_labels, true_labels)\n",
    "\n",
    "    num_penalties, thresh_low, thresh_high = \\\n",
    "        StatsUtils.yolo_oracle(Y_test[:, 1], proba[:, 1])\n",
    "    windowed_acc, windowed_supp = StatsUtils.windowed_accuracy(predicted_labels, Y_test)\n",
    "\n",
    "    metrics = {'precision': precision,\n",
    "               'recall': recall,\n",
    "               'fbeta': fbeta,\n",
    "               'support': support,\n",
    "               'accuracy': accuracy,\n",
    "               'penalities': num_penalties,\n",
    "               'windowed_accuracy': windowed_acc,\n",
    "               'windowed_support': windowed_supp}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model_regression(model, X_test, Y_test, batch_size=256):\n",
    "    begin = time.time()\n",
    "    raw_predictions = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    end = time.time()\n",
    "    mse = sklearn.metrics.mean_squared_error(Y_test, raw_predictions)\n",
    "\n",
    "    Y_classes = Y_test > 0.2 # FIXME\n",
    "    Y_classes = np.concatenate([1 - Y_classes, Y_classes], axis=1)\n",
    "\n",
    "    best = {'accuracy': 0}\n",
    "    for cutoff in np.arange(0.01, 0.75, 0.01):\n",
    "        predictions = raw_predictions > cutoff # FIXME\n",
    "        proba = np.concatenate([1 - predictions, predictions], axis=1)\n",
    "        metrics = stats_from_proba(proba, Y_classes)\n",
    "        metrics['cutoff'] = cutoff\n",
    "        print 'Cutoff: %f, metrics: %s' % (cutoff, str(metrics))\n",
    "        if metrics['accuracy'] > best['accuracy']:\n",
    "            best = metrics\n",
    "\n",
    "    metrics = best\n",
    "    metrics['mse'] = mse\n",
    "    metrics['test_time'] = end - begin\n",
    "    return metrics\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, batch_size=256):\n",
    "    predicted_labels, test_time = get_labels(model, X_test, batch_size, True)\n",
    "    true_labels = Y_test.argmax(axis=-1)\n",
    "\n",
    "    confusion = sklearn.metrics.confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Minor smoothing to prevent division by 0 errors\n",
    "    TN = float(confusion[0][0]) + 1\n",
    "    FN = float(confusion[1][0]) + 1\n",
    "    TP = float(confusion[1][1]) + 1\n",
    "    FP = float(confusion[0][1]) + 1\n",
    "    metrics = {'recall': TP / (TP + FN),\n",
    "               'specificity': TN / (FP + TN),\n",
    "               'precision': TP / (TP + FP),\n",
    "               'npv':  TN / (TN + FN),\n",
    "               'fpr': FP / (FP + TN),\n",
    "               'fdr': FP / (FP + TP),\n",
    "               'fnr': FN / (FN + TP),\n",
    "               'accuracy': (TP + TN) / (TP + FP + TN + FN),\n",
    "               'f1': (2 * TP) / (2 * TP + FP + FN),\n",
    "               'test_time': test_time}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def learn_and_eval(model, data, nb_epoch=2, batch_size=128,\n",
    "        validation_data=(None, None)):\n",
    "    X_train, Y_train, X_test, Y_test = data\n",
    "    train_time = run_model(model, data, nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size, validation_data=validation_data)\n",
    "    metrics = evaluate_model(model, X_test, Y_test, batch_size=batch_size)\n",
    "    return train_time, metrics\n",
    "\n",
    "\n",
    "# NOTE: assumes first two parameters are: (image_size, nb_classes)\n",
    "def try_params(model_gen, params, data,\n",
    "               output_dir, base_fname, model_name, OBJECT,\n",
    "               regression=False, nb_epoch=2, validation_data=(None, None)):\n",
    "    def metrics_names(metrics):\n",
    "        return sorted(metrics.keys())\n",
    "    def metrics_to_list(metrics):\n",
    "        return map(lambda key: metrics[key], metrics_names(metrics))\n",
    "\n",
    "    summary_csv_fname = os.path.join(\n",
    "            output_dir, base_fname + '_' + model_name + '_summary.csv')\n",
    "\n",
    "    X_train, Y_train, X_test, Y_test = data\n",
    "    to_write = []\n",
    "    for param in params:\n",
    "        param_base_fname = base_fname + '_' + model_name + '_' + '_'.join(map(str, param[2:]))\n",
    "        model_fname = os.path.join(\n",
    "                output_dir, param_base_fname + '.h5')\n",
    "        csv_fname = os.path.join(\n",
    "                output_dir, param_base_fname + '.csv')\n",
    "\n",
    "        # Make, train, and evaluate the model\n",
    "        model = model_gen(*param, regression=regression)\n",
    "        if regression:\n",
    "            train_time = run_model(model, data, nb_epoch=nb_epoch,\n",
    "                    validation_data=validation_data)\n",
    "            metrics = evaluate_model_regression(model, X_test, Y_test)\n",
    "        else:\n",
    "            train_time, metrics = learn_and_eval(model, data,\n",
    "                    validation_data=validation_data)\n",
    "\n",
    "        # Output predictions and save the model\n",
    "        # Redo some computation to save my sanity\n",
    "        conf1 = model.predict(X_train, batch_size=256, verbose=0)\n",
    "        conf2 = model.predict(X_test,  batch_size=256, verbose=0)\n",
    "        conf = np.concatenate([conf1, conf2])\n",
    "        if len(conf.shape) > 1:\n",
    "            assert len(conf.shape) == 2\n",
    "            assert conf.shape[1] <= 2\n",
    "            if conf.shape[1] == 2:\n",
    "                conf = conf[:, 1]\n",
    "            else:\n",
    "                conf = np.ravel(conf)\n",
    "        confidences_to_csv(csv_fname, conf, OBJECT)\n",
    "        model.save(model_fname)\n",
    "\n",
    "        to_write.append(list(param[2:]) + [train_time] + metrics_to_list(metrics))\n",
    "        print ('params: ', param)\n",
    "        print ('training time: ', train_time)\n",
    "        print ('metrics: ', metrics)\n",
    "        print\n",
    "    print to_write\n",
    "    # First two params don't need to be written out\n",
    "    param_column_names = map(lambda i: 'param' + str(i), xrange(len(params[0]) - 2))\n",
    "    column_names = param_column_names + ['train_time'] + metrics_names(metrics)\n",
    "    output_csv(summary_csv_fname, to_write, column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp/shuffled_small_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def to_test_train(avg_fname, all_frames, all_counts, train_ratio=0.6):\n",
    "#     assert len(all_frames) == len(all_counts), 'Frame length should equal counts length'\n",
    "\n",
    "#     nb_classes = all_counts.max() + 1\n",
    "#     X = all_frames\n",
    "\n",
    "#     mean = np.mean(X, axis=0)\n",
    "#     np.save(avg_fname, mean)\n",
    "\n",
    "#     N = 150000\n",
    "#     # N = 500000\n",
    "\n",
    "#     '''pos_inds = np.random.permutation(np.where(all_counts.ravel() == 0))\n",
    "#     pos_inds = pos_inds[0, 0 : N/2]\n",
    "#     neg_inds = np.random.permutation(np.where(all_counts.ravel() == 1))\n",
    "#     neg_inds = neg_inds[0, 0 : N/2]\n",
    "#     print pos_inds.shape\n",
    "#     print neg_inds.shape\n",
    "#     p = np.concatenate([pos_inds, neg_inds])\n",
    "#     np.random.shuffle(p)'''\n",
    "\n",
    "#     p = np.random.permutation(len(all_counts))\n",
    "#     p = p[0:N]\n",
    "\n",
    "#     Y = np_utils.to_categorical(all_counts, nb_classes)\n",
    "#     X, Y = X[p], Y[p]\n",
    "#     X -= mean\n",
    "\n",
    "#     def split(arr):\n",
    "#         # 250 -> 100, 50, 100\n",
    "#         ind = int(len(arr) * train_ratio)\n",
    "#         if ind > 50000:\n",
    "#             ind = len(arr) - 50000\n",
    "#         return arr[:ind], arr[ind:]\n",
    "\n",
    "#     X_train, X_test = split(X)\n",
    "#     Y_train, Y_test = split(Y)\n",
    "\n",
    "#     return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# def get_data(csv_fname, video_fname, avg_fname,\n",
    "#              num_frames=None, start_frame=0,\n",
    "#              OBJECTS=['person'], resol=(50, 50),\n",
    "#              center=True, dtype='float32', train_ratio=0.6):\n",
    "#     def print_class_numbers(Y, nb_classes):\n",
    "#         classes = Y.argmax(axis=-1)\n",
    "#         for i in xrange(nb_classes):\n",
    "#             print 'class %d: %d' % (i, np.sum(classes == i))\n",
    "\n",
    "#     print '\\tParsing %s, extracting %s' % (csv_fname, str(OBJECTS))\n",
    "#     all_counts = get_binary(csv_fname, limit=num_frames, OBJECTS=OBJECTS, start=start_frame)\n",
    "#     print '\\tRetrieving all frames from %s' % video_fname\n",
    "#     all_frames = get_all_frames(\n",
    "#             len(all_counts), video_fname, scale=resol, start=start_frame)\n",
    "#     print '\\tSplitting data into training and test sets'\n",
    "#     X_train, X_test, Y_train, Y_test = to_test_train(avg_fname, all_frames, all_counts)\n",
    "\n",
    "#     nb_classes = all_counts.max() + 1\n",
    "#     print '(train) positive examples: %d, total examples: %d' % \\\n",
    "#         (np.count_nonzero(Y_train.argmax(axis=-1)),\n",
    "#          len(Y_train))\n",
    "#     print_class_numbers(Y_train, nb_classes)\n",
    "#     print '(test) positive examples: %d, total examples: %d' % \\\n",
    "#         (np.count_nonzero(Y_test.argmax(axis=-1)),\n",
    "#          len(Y_test))\n",
    "#     print_class_numbers(Y_test, nb_classes)\n",
    "\n",
    "#     print 'shape of image: ' + str(all_frames[0].shape)\n",
    "#     print 'number of classes: %d' % (nb_classes)\n",
    "\n",
    "#     data = (X_train, Y_train, X_test, Y_test)\n",
    "#     return data, nb_classes\n",
    "\n",
    "# def get_csv_adaptively(csv_fname, num_frames, OBJ, interval=1, start=0):\n",
    "#     df = pd.read_csv(csv_fname)\n",
    "#     df = df[df['object_name'].isin([OBJ])]\n",
    "#     groups = df.set_index('frame')\n",
    "#     end = start + num_frames * interval\n",
    "#     res = {i: i in groups.index for i in range(start, end, interval)}\n",
    "#     return res\n",
    "\n",
    "# def get_data_adaptively(csv_fname, video_fname, avg_fname,\n",
    "#              num_frames=None, start_frame=0,\n",
    "#              OBJECTS=['person'], resol=(50, 50),\n",
    "#              center=True, dtype='float32', train_ratio=0.6, burst=0, interval=1, decoded_images=False):\n",
    "#     def print_class_numbers(Y, nb_classes):\n",
    "#         classes = Y.argmax(axis=-1)\n",
    "#         for i in xrange(nb_classes):\n",
    "#             print 'class %d: %d' % (i, np.sum(classes == i))\n",
    "#     print '\\tParsing %s, extracting %s' % (csv_fname, str(OBJECTS))\n",
    "#     all_counts = get_csv_adaptively(csv_fname, num_frames, OBJECTS[0],\n",
    "#                                     start=start_frame, burst=burst, interval=interval)\n",
    "#     print '\\tCutting frames %d %d' % (len(all_counts.keys()), num_frames)\n",
    "#     all_counts = sorted(all_counts.items(), key=lambda x: x[0])[:num_frames]\n",
    "#     all_indexs = [x[0] for x in all_counts]\n",
    "#     all_labels = np.array([x[1] for x in all_counts])\n",
    "#     print '\\tRetrieving all frames from %s' % video_fname\n",
    "#     if not decoded_images:\n",
    "#         all_frames = get_all_frames(\n",
    "#             num_frames, video_fname, scale=resol, start=start_frame, frameset=all_indexs)\n",
    "#     else:\n",
    "#         all_frames = get_frames_from_images(all_indexs, video_fname, resol=resol, start=start_frame)\n",
    "#     print '\\tSplitting data into training and test sets'\n",
    "#     X_train, X_test, Y_train, Y_test = to_test_train(avg_fname, all_frames, all_labels)\n",
    "\n",
    "#     nb_classes = all_labels.max() + 1\n",
    "#     print '(train) positive examples: %d, total examples: %d' % \\\n",
    "#         (np.count_nonzero(Y_train.argmax(axis=-1)),\n",
    "#          len(Y_train))\n",
    "#     print_class_numbers(Y_train, nb_classes)\n",
    "#     print '(test) positive examples: %d, total examples: %d' % \\\n",
    "#         (np.count_nonzero(Y_test.argmax(axis=-1)),\n",
    "#          len(Y_test))\n",
    "#     print_class_numbers(Y_test, nb_classes)\n",
    "\n",
    "#     print 'shape of image: ' + str(all_frames[0].shape)\n",
    "#     print 'number of classes: %d' % (nb_classes)\n",
    "\n",
    "#     data = (X_train, Y_train, X_test, Y_test)\n",
    "#     return data, nb_classes\n",
    "\n",
    "def to_test_train(all_frames, all_labels, avg_num=[], nb_classes=2):\n",
    "    assert len(all_frames) == len(all_labels), 'Frame length should equal counts length'\n",
    "    X = all_frames\n",
    "    if len(avg_num) == 0:\n",
    "        mean = np.mean(X, axis=0)\n",
    "    else:\n",
    "        mean = avg_num\n",
    "    Y = np_utils.to_categorical(all_labels, nb_classes)\n",
    "    X -= mean\n",
    "\n",
    "    return X, Y, mean\n",
    "\n",
    "def get_csv_samples(csv_fname, OBJ):\n",
    "    df = pd.read_csv(csv_fname)\n",
    "    df = df[df['object_name'].isin([OBJ])]\n",
    "    groups = df.set_index('frame')\n",
    "    return groups.index\n",
    "\n",
    "def get_labels(csv_res, frameset):\n",
    "    ret = [t in csv_res for t in frameset]\n",
    "    print ('reading label sum: %d, pos: %d' % (len(ret), sum(ret)))\n",
    "    return np_utils.to_categorical(ret, 2)\n",
    "\n",
    "def get_frames_from_images(frameset, video_fname, resol=(50, 50), start=0, dtype='float32'):\n",
    "    if frameset == None:\n",
    "        print 'ERROR: frameset is none!'\n",
    "        sys.exit()\n",
    "    print ('reading images... %d') % (len(frameset))\n",
    "    frames = np.zeros( tuple([len(frameset)] + list(resol) + [3]), dtype=dtype )\n",
    "    for i in range(len(frameset)):\n",
    "#         if i % 1000 == 0:\n",
    "#             sys.stdout.write(\"\\033[F\") # Cursor up one line\n",
    "#             clear_output(wait=True)\n",
    "#             display('Iteration '+str(i)+' Score: '+str(uniform(0, 1)))\n",
    "#             display('\\timages read %d/%d' % (i, len(frameset))\n",
    "        img_path = os.path.join(video_fname, str(frameset[i] + 1).zfill(7) + '.jpg')\n",
    "        frame = cv2.imread(img_path)\n",
    "        frame = cv2.resize(frame, resol, interpolation=cv2.INTER_NEAREST)\n",
    "        frames[i, :] = frame\n",
    "\n",
    "    if dtype == 'float32':\n",
    "        frames /= 255.0\n",
    "\n",
    "    return frames\n",
    "\n",
    "def get_train_test_data(csv_fname, video_fname, train_nums, OBJ, pos_train_ratio=None):\n",
    "    total_sample = range(0, 3000000)\n",
    "    total_sample_num = len(total_sample)\n",
    "    print ('reading csv file: %s %s' % (csv_fname, OBJ))\n",
    "    csv_ret = get_csv_samples(csv_fname, OBJ)\n",
    "    total_pos_sample = [t for t in total_sample if t in csv_ret]\n",
    "    total_pos_sample_num = len(total_pos_sample)\n",
    "    total_neg_sample = [t for t in total_sample if t not in csv_ret]\n",
    "    total_neg_sample_num = len(total_neg_sample)\n",
    "    print ('total sample: %d, positive sample: %d' % (total_sample_num, total_pos_sample_num))\n",
    "    if pos_train_ratio != None:\n",
    "        pos_train_nums = int(train_nums * pos_train_ratio)\n",
    "    else:\n",
    "        pos_train_nums = int(train_nums * (float(total_pos_sample_num) / total_sample_num))\n",
    "    print ('training sample: %d, positive training sample: %d' % (train_nums, pos_train_nums))\n",
    "#     print csv_ret\n",
    "    \n",
    "    # get training samples\n",
    "    pos_frame_ids = np.random.permutation(total_pos_sample)\n",
    "    pos_frame_ids = pos_frame_ids[0:pos_train_nums]\n",
    "    neg_frame_ids = np.random.permutation(total_neg_sample)\n",
    "    neg_frame_ids = neg_frame_ids[0:train_nums - pos_train_nums]\n",
    "    train_frame_ids = np.concatenate((pos_frame_ids, neg_frame_ids), axis=0)\n",
    "    train_frame_ids = sorted(train_frame_ids)\n",
    "    print ('reading training image files...%d' % (len(train_frame_ids)))\n",
    "    train_frames = get_frames_from_images(train_frame_ids, video_fname)\n",
    "    train_labels = np.array([i in csv_ret for i in train_frame_ids])\n",
    "    X_train, Y_train, mean = to_test_train(train_frames, train_labels)\n",
    "    \n",
    "    # get testing samples\n",
    "    test_frame_ids = range(0, total_sample_num, 30)\n",
    "    print ('reading testing image files...%d' % (len(test_frame_ids)))\n",
    "    test_frames = get_frames_from_images(test_frame_ids, video_fname)\n",
    "    test_labels = [i in total_pos_sample for i in test_frame_ids]\n",
    "#     print len(test_frames), len(test_labels)\n",
    "    X_test, Y_test, _ = to_test_train(test_frames, test_labels, avg_num=mean)\n",
    "    print ('testing sample: %d, positive testing sample: %d' %\n",
    "           (len(test_frames), np.count_nonzero(Y_test.argmax(axis=-1))))\n",
    "    return [X_train, Y_train, X_test, Y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Run Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_path = '/host/hybridvs_data/'\n",
    "# # video_name = 'coral-reef-long'\n",
    "# # OBJECT = 'person'\n",
    "# video_name = 'jackson-town-square'\n",
    "# OBJECT = 'bus'\n",
    "# csv_in = os.path.join(data_path, 'csv', video_name + '.csv')\n",
    "# video_in = os.path.join(data_path, 'videos/scaled_50X50', video_name + '.mp4')\n",
    "# output_dir = os.path.join(data_path, 'cnn-models')\n",
    "# avg_fname = video_name + '.npy'\n",
    "# start_frame = 0\n",
    "# resol = (50, 50)\n",
    "# objects = [OBJECT]\n",
    "# split_image_path = '/host/mengwei/frames_50X50_jackson-town-square/'\n",
    "# num_frames = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print 'Preparing data....'\n",
    "# gc.collect()\n",
    "# data = get_train_test_data(\n",
    "#         csv_in, split_image_path, num_frames, OBJECT, pos_train_ratio=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, batch_size=128,\n",
    "              regression=False, nb_epoch=2, validation_data=(None, None)):\n",
    "    train_time = run_model(model, data, nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size, validation_data=validation_data)\n",
    "    return train_time, model\n",
    "# def test_model_two_thre(model, data, batch_size=128, error_rate=1):\n",
    "#     X_train, Y_train, X_test, Y_test = data\n",
    "#     probs = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "#     pos_samples = np.count_nonzero(Y_test.argmax(axis=-1))\n",
    "#     error_tolerate = pos_samples * (error_rate / 100.0)\n",
    "#     total_test_num = probs.shape[0]\n",
    "#     print ('test num: %d, pos num: %d, tolerate: %d' % (total_test_num, pos_samples, error_tolerate))\n",
    "#     combined_res = np.column_stack((probs[:,1], Y_test[:,1]))\n",
    "#     combined_res = combined_res[combined_res[:,0].argsort()]\n",
    "#     print combined_res\n",
    "#     # get T_low\n",
    "#     _error = 0\n",
    "#     for i in range(total_test_num):\n",
    "#         _error += combined_res[i,1]\n",
    "#         if _error > error_tolerate:\n",
    "#             _error -= 1\n",
    "#             T_low = combined_res[i - 1, 0]\n",
    "#             pos_low = i - 1\n",
    "#             break\n",
    "#     # get T_high\n",
    "#     _error = 0\n",
    "#     for i in reversed(range(total_test_num)):\n",
    "#         _error += (1 - combined_res[i,1])\n",
    "#         if _error > error_tolerate:\n",
    "#             _error -= 1\n",
    "#             T_high = combined_res[i + 1, 0]\n",
    "#             pos_high = i + 1\n",
    "#             break\n",
    "#     filter_ratio = 1 - float(pos_high - pos_low) / total_test_num\n",
    "#     return filter_ratio, float(_error) / pos_samples, T_low, T_high\n",
    "# return list(threshold, recall, precision)\n",
    "def test_model_one_thre(model, data, batch_size=128):\n",
    "    _, _, X_test, Y_test = data\n",
    "    probs = model.predict(X_test, batch_size=128, verbose=0)\n",
    "    pos_samples = np.count_nonzero(Y_test.argmax(axis=-1))\n",
    "    total_test_num = probs.shape[0]\n",
    "    print ('test num: %d, pos num: %d' % (total_test_num, pos_samples))\n",
    "    combined_res = np.column_stack((probs[:,1], Y_test[:,1]))\n",
    "    combined_res = combined_res[combined_res[:,0].argsort()]\n",
    "    print combined_res\n",
    "    # recall = TP / (TP + FN)\n",
    "    # precision = TP / (TP + FP)\n",
    "    ret = []\n",
    "    FN = 0\n",
    "    for i in range(total_test_num - 1):\n",
    "        FN += combined_res[i,1]\n",
    "        TP = pos_samples - FN\n",
    "        FP = total_test_num - i - 1 - TP\n",
    "        recall = float(TP) / (TP + FN)\n",
    "        precision = float(TP) / (TP + FP)\n",
    "#         print FN, TP, FP\n",
    "        if len(ret) > 0 and ret[-1][1] == recall:\n",
    "            ret[-1][2] = max(precision, ret[-1][2])\n",
    "        else:\n",
    "            ret.append([combined_res[i,0], recall, precision])\n",
    "    return ret\n",
    "# def try_model(model, data, OBJECT):\n",
    "#     train_time, model = train_model(model, data, OBJECT)\n",
    "#     filter_ratio, error, T_low, T_high = test_model(model, data)\n",
    "#     print train_time, filter_ratio, error, T_low, T_high\n",
    "def try_model_one_thre(model, data, nb_epoch=2, batch_size=128):\n",
    "    train_time, model = train_model(model, data, nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "    ret = test_model_one_thre(model, data)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare the data for training and testing\n",
    "# data = [X_train, Y_train, X_test, Y_test]\n",
    "# X_train and X_test: np.array([N X 50 X 50 X 3]), using the function get_frames_from_images above\n",
    "# Y_train and Y_test: np.array([N X 2]), the second column is the chance of positive object,\n",
    "# for example: Y_train=np.array([[0, 1], [1, 0]]), the first sample contains the object, the second doesn't\n",
    "\n",
    "resol=(50, 50)\n",
    "#frameset=None\n",
    "# Data comes from ILSVRC2014, in /media/teddyxu/blue-2tb\n",
    "# n02924116 are bus images, can change as needed\n",
    "pos_names = os.listdir(\"/host/hybridvs_data/ILSVRC2014_DET_train/n02924116/\")\n",
    "frames = np.zeros(tuple([1000] + list(resol) + [3]), dtype='float32')\n",
    "\n",
    "# Read positive images from the disk\n",
    "for i in range(0, len(pos_names)):\n",
    "    #print len(pos_names)\n",
    "    pos_img_path = os.path.join('/host/hybridvs_data/ILSVRC2014_DET_train/n02924116/', pos_names[i])\n",
    "    #print pos_img_path\n",
    "    frame = cv2.imread(pos_img_path)\n",
    "    frame = cv2.resize(frame, resol, interpolation=cv2.INTER_NEAREST)\n",
    "    frames[i,:] = frame\n",
    "    \n",
    "#pos_frames /= 255.0\n",
    "\n",
    "# Read negative images from the disk\n",
    "neg_dir = os.listdir(\"/host/hybridvs_data/ILSVRC2014_DET_train/\")\n",
    "for i in range(0, 500):\n",
    "    # True labels\n",
    "    if neg_dir[i] == 'n02924116':\n",
    "        continue\n",
    "    # False Labels, Randomly picked from other directories\n",
    "    neg_names = os.listdir(os.path.join('/host/hybridvs_data/ILSVRC2014_DET_train/', neg_dir[i]))\n",
    "    neg_img_path = os.path.join('/host/hybridvs_data/ILSVRC2014_DET_train/', neg_dir[i], neg_names[0])\n",
    "    #print neg_img_path\n",
    "    frame = cv2.imread(neg_img_path)\n",
    "    frame = cv2.resize(frame, resol, interpolation=cv2.INTER_NEAREST)\n",
    "    frames[i + 500,:] = frame\n",
    "\n",
    "frames /= 255.0\n",
    "\n",
    "Y = []\n",
    "for i in range(0, 1000):\n",
    "    if i < 500:\n",
    "        Y.append([0,1])\n",
    "    else:\n",
    "        Y.append([1,0])\n",
    "\n",
    "#print Y\n",
    "\n",
    "list_seq = range(0, 1000)\n",
    "#print list_seq\n",
    "random.shuffle(list_seq)\n",
    "#print list_seq\n",
    "\n",
    "#print len(frames)\n",
    "\n",
    "X_train = np.random.rand(1000, 50, 50, 3)\n",
    "Y_train = np.random.randint(2, size=(1000, 2))\n",
    "X_test = np.random.rand(1000, 50, 50, 3)\n",
    "Y_test = np.random.randint(2, size=(1000, 2))\n",
    "\n",
    "# First 800 samples as training set\n",
    "for i in range(0, 800):\n",
    "    X_train[i] = frames[list_seq[i]]\n",
    "    Y_train[i] = Y[list_seq[i]]\n",
    "\n",
    "# First 200 samples as training set\n",
    "for i in range(0, 200):\n",
    "    X_test[i] = frames[list_seq[i + 800]]\n",
    "    Y_test[i] = Y[list_seq[i + 800]]\n",
    "    \n",
    "data = [X_train, Y_train, X_test, Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "training samples: 452/1000, testing samples: 312/1000\n",
      "shape:  (1000, 50, 50, 3) (1000, 2) (1000, 50, 50, 3) (1000, 2)\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3064: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), activation=\"relu\", input_shape=(50, 50, 3...)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:99: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.8056 - acc: 0.5270 - mean_squared_error: 0.2833\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 141us/step - loss: 0.6736 - acc: 0.5910 - mean_squared_error: 0.2380\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 143us/step - loss: 0.7424 - acc: 0.6770 - mean_squared_error: 0.2484\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 140us/step - loss: 0.6401 - acc: 0.6760 - mean_squared_error: 0.2182\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 142us/step - loss: 0.5661 - acc: 0.7300 - mean_squared_error: 0.1900\n",
      "test num: 1000, pos num: 312\n",
      "[[0.04976433 0.        ]\n",
      " [0.10646225 0.        ]\n",
      " [0.12103866 0.        ]\n",
      " ...\n",
      " [0.9674595  1.        ]\n",
      " [0.96769851 1.        ]\n",
      " [0.96915543 1.        ]]\n",
      "precision 0.99 [0.3766624629497528, 0.9903846153846154, 0.3256059009483667]\n",
      "precision 0.95 [0.5448015928268433, 0.967948717948718, 0.3296943231441048]\n",
      "precision 0.9 [0.5448015928268433, 0.967948717948718, 0.3296943231441048]\n"
     ]
    }
   ],
   "source": [
    "# after data prepared, just run this cell\n",
    "K.clear_session()\n",
    "param = ((50, 50, 3), 2, 128, 32, 2)\n",
    "model = generate_conv_net_base(*param, regression=False)\n",
    "accuracy = try_model_one_thre(model, data, nb_epoch=5)\n",
    "RECALL_LIST = [0.99, 0.95, 0.90]\n",
    "for r in RECALL_LIST:\n",
    "    temp = [a for a in accuracy if a[1] >= r]\n",
    "    item = max(temp, key=lambda x: x[2])\n",
    "    print 'precision', r, item\n",
    "model.save('/host/hybridvs_data/models/imagenet_init_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test V0 on jackson-town-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading csv file: /host/hybridvs_data/csv/jackson-town-square.csv, object: bus\n",
      "45499 2954501\n"
     ]
    }
   ],
   "source": [
    "v0_model = load_model('/host/hybridvs_data/models/imagenet_init_model.h5')\n",
    "OBJ = 'bus'\n",
    "csv_fname = '/host/hybridvs_data/csv/jackson-town-square.csv'\n",
    "total_sample = range(0, 3000000)\n",
    "total_sample_num = len(total_sample)\n",
    "print ('reading csv file: %s, object: %s' % (csv_fname, OBJ))\n",
    "\n",
    "csv_ret = get_csv_samples(csv_fname, OBJ)\n",
    "total_pos_sample = [t for t in total_sample if t in csv_ret]\n",
    "total_pos_sample_num = len(total_pos_sample)\n",
    "total_neg_sample = [t for t in total_sample if t not in csv_ret]\n",
    "total_neg_sample_num = len(total_neg_sample)\n",
    "print total_pos_sample_num, total_neg_sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.03336809176225235 \t1.0 \t0.2259233\n",
      "0.032359081419624215 \t0.96875 \t0.22606355\n",
      "0.03239289446185998 \t0.96875 \t0.22701654\n",
      "0.032426778242677826 \t0.96875 \t0.22705653\n",
      "0.032460732984293195 \t0.96875 \t0.22716673\n",
      "0.03249475890985325 \t0.96875 \t0.22722559\n",
      "0.03252885624344176 \t0.96875 \t0.22761054\n",
      "0.032563025210084036 \t0.96875 \t0.2278558\n",
      "0.03259726603575184 \t0.96875 \t0.22854704\n",
      "0.03263157894736842 \t0.96875 \t0.22855279\n",
      "0.032665964172813484 \t0.96875 \t0.22859856\n",
      "0.03270042194092827 \t0.96875 \t0.22863469\n",
      "0.03273495248152059 \t0.96875 \t0.22864217\n",
      "0.03276955602536998 \t0.96875 \t0.22873399\n",
      "0.0328042328042328 \t0.96875 \t0.22873873\n",
      "0.03283898305084746 \t0.96875 \t0.22887027\n",
      "0.032873806998939555 \t0.96875 \t0.22889856\n",
      "0.032908704883227176 \t0.96875 \t0.22894673\n",
      "0.03294367693942614 \t0.96875 \t0.22903623\n",
      "0.03297872340425532 \t0.96875 \t0.22906736\n",
      "0.03301384451544196 \t0.96875 \t0.22923155\n",
      "0.03304904051172708 \t0.96875 \t0.22924204\n",
      "0.033084311632870865 \t0.96875 \t0.22926085\n",
      "0.03311965811965812 \t0.96875 \t0.2292748\n",
      "0.033155080213903745 \t0.96875 \t0.2293\n",
      "0.033190578158458245 \t0.96875 \t0.22930187\n",
      "0.03322615219721329 \t0.96875 \t0.22932014\n",
      "0.033261802575107295 \t0.96875 \t0.22932494\n",
      "0.03329752953813104 \t0.96875 \t0.22933048\n",
      "0.03333333333333333 \t0.96875 \t0.2293567\n",
      "0.03336921420882669 \t0.96875 \t0.22938333\n",
      "0.0334051724137931 \t0.96875 \t0.22946543\n",
      "0.03344120819848975 \t0.96875 \t0.22967497\n",
      "0.03347732181425486 \t0.96875 \t0.22999313\n",
      "0.03351351351351351 \t0.96875 \t0.23005314\n",
      "0.03354978354978355 \t0.96875 \t0.23009804\n",
      "0.03358613217768147 \t0.96875 \t0.23016118\n",
      "0.033659066232356136 \t0.96875 \t0.23023015\n",
      "0.03369565217391304 \t0.96875 \t0.23028934\n",
      "0.03373231773667029 \t0.96875 \t0.23036942\n",
      "0.03376906318082789 \t0.96875 \t0.23046903\n",
      "0.03380588876772083 \t0.96875 \t0.23059915\n",
      "0.03384279475982533 \t0.96875 \t0.23065138\n",
      "0.033879781420765025 \t0.96875 \t0.23087433\n",
      "0.03391684901531729 \t0.96875 \t0.2308938\n",
      "0.033953997809419496 \t0.96875 \t0.23095697\n",
      "0.03399122807017544 \t0.96875 \t0.23100726\n",
      "0.03402854006586169 \t0.96875 \t0.23102282\n",
      "0.03406593406593406 \t0.96875 \t0.23109472\n",
      "0.034103410341034104 \t0.96875 \t0.23132645\n",
      "0.034140969162995596 \t0.96875 \t0.23136516\n",
      "0.034178610804851156 \t0.96875 \t0.23146352\n",
      "0.03421633554083885 \t0.96875 \t0.23148361\n",
      "0.03425414364640884 \t0.96875 \t0.23153475\n",
      "0.034292035398230086 \t0.96875 \t0.231551\n",
      "0.03433001107419712 \t0.96875 \t0.23170415\n",
      "0.03436807095343681 \t0.96875 \t0.2317323\n",
      "0.03440621531631521 \t0.96875 \t0.23176454\n",
      "0.034444444444444444 \t0.96875 \t0.23185892\n",
      "0.034482758620689655 \t0.96875 \t0.23205404\n",
      "0.034521158129175944 \t0.96875 \t0.23212038\n",
      "0.03455964325529543 \t0.96875 \t0.232145\n",
      "0.03459821428571429 \t0.96875 \t0.23228768\n",
      "0.034636871508379886 \t0.96875 \t0.23235723\n",
      "0.03467561521252797 \t0.96875 \t0.23239335\n",
      "0.03471444568868981 \t0.96875 \t0.23244299\n",
      "0.034753363228699555 \t0.96875 \t0.23246229\n",
      "0.03479236812570146 \t0.96875 \t0.2325163\n",
      "0.0348314606741573 \t0.96875 \t0.23254327\n",
      "0.03487064116985377 \t0.96875 \t0.23257896\n",
      "0.03490990990990991 \t0.96875 \t0.23259312\n",
      "0.03494926719278467 \t0.96875 \t0.23260234\n",
      "0.034988713318284424 \t0.96875 \t0.23263009\n",
      "0.03502824858757062 \t0.96875 \t0.23263131\n",
      "0.03506787330316742 \t0.96875 \t0.23271371\n",
      "0.035107587768969425 \t0.96875 \t0.23271428\n",
      "0.035147392290249435 \t0.96875 \t0.23272766\n",
      "0.03518728717366629 \t0.96875 \t0.23273757\n",
      "0.035227272727272725 \t0.96875 \t0.23280941\n",
      "0.03526734926052332 \t0.96875 \t0.23286448\n",
      "0.03530751708428246 \t0.96875 \t0.2328744\n",
      "0.03534777651083238 \t0.96875 \t0.23289445\n",
      "0.03538812785388128 \t0.96875 \t0.23293836\n",
      "0.03542857142857143 \t0.96875 \t0.23295064\n",
      "0.03546910755148741 \t0.96875 \t0.2329556\n",
      "0.035509736540664374 \t0.96875 \t0.23295932\n",
      "0.03555045871559633 \t0.96875 \t0.23298055\n",
      "0.03559127439724455 \t0.96875 \t0.23301888\n",
      "0.035632183908045977 \t0.96875 \t0.23306897\n",
      "0.03567318757192175 \t0.96875 \t0.2331344\n",
      "0.03571428571428571 \t0.96875 \t0.23314598\n",
      "0.03575547866205306 \t0.96875 \t0.23315421\n",
      "0.03579676674364896 \t0.96875 \t0.23336235\n",
      "0.035838150289017344 \t0.96875 \t0.2333817\n",
      "0.03587962962962963 \t0.96875 \t0.23339991\n",
      "0.03592120509849363 \t0.96875 \t0.23341414\n",
      "0.03596287703016241 \t0.96875 \t0.23342292\n",
      "0.036004645760743324 \t0.96875 \t0.23343077\n",
      "0.03604651162790698 \t0.96875 \t0.2334721\n",
      "0.03608847497089639 \t0.96875 \t0.23348045\n",
      "0.03613053613053613 \t0.96875 \t0.23348565\n",
      "0.03617269544924154 \t0.96875 \t0.2335348\n",
      "0.036214953271028034 \t0.96875 \t0.23378156\n",
      "0.03625730994152047 \t0.96875 \t0.23380388\n",
      "0.03629976580796253 \t0.96875 \t0.23381329\n",
      "0.03634232121922626 \t0.96875 \t0.23385644\n",
      "0.036384976525821594 \t0.96875 \t0.23392479\n",
      "0.036427732079905996 \t0.96875 \t0.23397481\n",
      "0.036470588235294116 \t0.96875 \t0.23399091\n",
      "0.03651354534746761 \t0.96875 \t0.23401417\n",
      "0.036556603773584904 \t0.96875 \t0.23409364\n",
      "0.03659976387249114 \t0.96875 \t0.23415327\n",
      "0.03664302600472813 \t0.96875 \t0.2341984\n",
      "0.036686390532544376 \t0.96875 \t0.2342528\n",
      "0.03672985781990521 \t0.96875 \t0.2342842\n",
      "0.036773428232502965 \t0.96875 \t0.23431869\n",
      "0.03681710213776722 \t0.96875 \t0.23436159\n",
      "0.036860879904875146 \t0.96875 \t0.23438658\n",
      "0.036904761904761905 \t0.96875 \t0.2344607\n",
      "0.03694874851013111 \t0.96875 \t0.23451239\n",
      "0.03699284009546539 \t0.96875 \t0.23452157\n",
      "0.037037037037037035 \t0.96875 \t0.23456335\n",
      "0.03708133971291866 \t0.96875 \t0.23460424\n",
      "0.037125748502994015 \t0.96875 \t0.23467286\n",
      "0.03717026378896882 \t0.96875 \t0.23468576\n",
      "0.03721488595438175 \t0.96875 \t0.23469938\n",
      "0.037259615384615384 \t0.96875 \t0.23471074\n",
      "0.03730445246690734 \t0.96875 \t0.23474267\n",
      "0.03734939759036145 \t0.96875 \t0.23474586\n",
      "0.03739445114595899 \t0.96875 \t0.23478693\n",
      "0.03743961352657005 \t0.96875 \t0.23479073\n",
      "0.037484885126964934 \t0.96875 \t0.23479368\n",
      "0.03753026634382567 \t0.96875 \t0.23480491\n",
      "0.037575757575757575 \t0.96875 \t0.23486286\n",
      "0.03762135922330097 \t0.96875 \t0.23498909\n",
      "0.03766707168894289 \t0.96875 \t0.23505288\n",
      "0.037712895377128956 \t0.96875 \t0.23533128\n",
      "0.037758830694275276 \t0.96875 \t0.23540972\n",
      "0.03780487804878049 \t0.96875 \t0.23542662\n",
      "0.03785103785103785 \t0.96875 \t0.23548388\n",
      "0.037897310513447434 \t0.96875 \t0.2355292\n",
      "0.037943696450428395 \t0.96875 \t0.23552997\n",
      "0.03799019607843137 \t0.96875 \t0.2355367\n",
      "0.03803680981595092 \t0.96875 \t0.23559442\n",
      "0.038083538083538086 \t0.96875 \t0.23559809\n",
      "0.038130381303813035 \t0.96875 \t0.23570938\n",
      "0.038177339901477834 \t0.96875 \t0.23571572\n",
      "0.03822441430332922 \t0.96875 \t0.23575564\n",
      "0.03827160493827161 \t0.96875 \t0.23581801\n",
      "0.038318912237330034 \t0.96875 \t0.23582266\n",
      "0.038366336633663366 \t0.96875 \t0.23582417\n",
      "0.03841387856257745 \t0.96875 \t0.23598102\n",
      "0.038461538461538464 \t0.96875 \t0.23603836\n",
      "0.03850931677018633 \t0.96875 \t0.23606572\n",
      "0.03855721393034826 \t0.96875 \t0.23608893\n",
      "0.038605230386052306 \t0.96875 \t0.23613685\n",
      "0.03865336658354115 \t0.96875 \t0.23613872\n",
      "0.03870162297128589 \t0.96875 \t0.23614635\n",
      "0.03875 \t0.96875 \t0.23623312\n",
      "0.03879849812265332 \t0.96875 \t0.23626074\n",
      "0.03884711779448621 \t0.96875 \t0.23641129\n",
      "0.03889585947302384 \t0.96875 \t0.23645686\n",
      "0.038944723618090454 \t0.96875 \t0.23668487\n",
      "0.0389937106918239 \t0.96875 \t0.23678812\n",
      "0.03904282115869018 \t0.96875 \t0.23683016\n",
      "0.03909205548549811 \t0.96875 \t0.23684229\n",
      "0.039141414141414144 \t0.96875 \t0.23687623\n",
      "0.039190897597977246 \t0.96875 \t0.23695764\n",
      "0.039240506329113925 \t0.96875 \t0.2369581\n",
      "0.03929024081115336 \t0.96875 \t0.23698536\n",
      "0.03934010152284264 \t0.96875 \t0.23704773\n",
      "0.03939008894536213 \t0.96875 \t0.23710151\n",
      "0.039440203562340966 \t0.96875 \t0.23710155\n",
      "0.03821656050955414 \t0.9375 \t0.23714015\n",
      "0.03826530612244898 \t0.9375 \t0.23723933\n",
      "0.038314176245210725 \t0.9375 \t0.23730558\n",
      "0.03836317135549872 \t0.9375 \t0.23738746\n",
      "0.03841229193341869 \t0.9375 \t0.23745425\n",
      "0.038461538461538464 \t0.9375 \t0.23747861\n",
      "0.038510911424903725 \t0.9375 \t0.23748514\n",
      "0.038560411311053984 \t0.9375 \t0.2375241\n",
      "0.03861003861003861 \t0.9375 \t0.23754518\n",
      "0.03865979381443299 \t0.9375 \t0.23773628\n",
      "0.03870967741935484 \t0.9375 \t0.23773806\n",
      "0.03875968992248062 \t0.9375 \t0.23775452\n",
      "0.03880983182406209 \t0.9375 \t0.23775865\n",
      "0.03756476683937824 \t0.90625 \t0.23778524\n",
      "0.03761348897535668 \t0.90625 \t0.23799066\n",
      "0.03766233766233766 \t0.90625 \t0.23801112\n",
      "0.0377113133940182 \t0.90625 \t0.23801182\n",
      "0.037760416666666664 \t0.90625 \t0.23815623\n",
      "0.03780964797913951 \t0.90625 \t0.23818542\n",
      "0.037859007832898174 \t0.90625 \t0.238199\n",
      "0.036601307189542485 \t0.875 \t0.23827551\n",
      "0.03664921465968586 \t0.875 \t0.23835263\n",
      "0.03669724770642202 \t0.875 \t0.23842058\n",
      "0.03674540682414698 \t0.875 \t0.23842417\n",
      "0.03679369250985545 \t0.875 \t0.2384428\n",
      "0.03684210526315789 \t0.875 \t0.23846017\n",
      "0.03689064558629776 \t0.875 \t0.23849109\n",
      "0.036939313984168866 \t0.875 \t0.23855068\n",
      "0.036988110964332896 \t0.875 \t0.2385649\n",
      "0.037037037037037035 \t0.875 \t0.23860788\n",
      "0.03708609271523179 \t0.875 \t0.23862377\n",
      "0.03713527851458886 \t0.875 \t0.2386249\n",
      "0.03718459495351926 \t0.875 \t0.23866107\n",
      "0.03723404255319149 \t0.875 \t0.23866388\n",
      "0.037283621837549935 \t0.875 \t0.23866473\n",
      "0.037333333333333336 \t0.875 \t0.23866719\n",
      "0.037383177570093455 \t0.875 \t0.23867682\n",
      "0.0374331550802139 \t0.875 \t0.23870036\n",
      "0.03748326639892905 \t0.875 \t0.2387396\n",
      "0.03753351206434316 \t0.875 \t0.23875944\n",
      "0.03758389261744966 \t0.875 \t0.23878548\n",
      "0.03763440860215054 \t0.875 \t0.2387892\n",
      "0.03768506056527591 \t0.875 \t0.2388372\n",
      "0.03773584905660377 \t0.875 \t0.23884185\n",
      "0.037786774628879895 \t0.875 \t0.23890106\n",
      "0.03783783783783784 \t0.875 \t0.23894958\n",
      "0.037889039242219216 \t0.875 \t0.23904146\n",
      "0.037940379403794036 \t0.875 \t0.23913844\n",
      "0.037991858887381276 \t0.875 \t0.23917025\n",
      "0.03804347826086957 \t0.875 \t0.23922661\n",
      "0.0380952380952381 \t0.875 \t0.23924343\n",
      "0.03814713896457766 \t0.875 \t0.23924473\n",
      "0.03819918144611187 \t0.875 \t0.2393588\n",
      "0.03825136612021858 \t0.875 \t0.23937684\n",
      "0.036935704514363885 \t0.84375 \t0.23937789\n",
      "0.036986301369863014 \t0.84375 \t0.23937947\n",
      "0.037037037037037035 \t0.84375 \t0.23937964\n",
      "0.03708791208791209 \t0.84375 \t0.23944245\n",
      "0.037138927097661624 \t0.84375 \t0.23973818\n",
      "0.0371900826446281 \t0.84375 \t0.23978685\n",
      "0.037241379310344824 \t0.84375 \t0.23979948\n",
      "0.03729281767955801 \t0.84375 \t0.23987086\n",
      "0.03734439834024896 \t0.84375 \t0.2399719\n",
      "0.037396121883656507 \t0.84375 \t0.2401461\n",
      "0.03744798890429958 \t0.84375 \t0.24021363\n",
      "0.0375 \t0.84375 \t0.24027236\n",
      "0.037552155771905425 \t0.84375 \t0.24030472\n",
      "0.037604456824512536 \t0.84375 \t0.24033673\n",
      "0.03765690376569038 \t0.84375 \t0.24033883\n",
      "0.03770949720670391 \t0.84375 \t0.24037695\n",
      "0.03776223776223776 \t0.84375 \t0.24040116\n",
      "0.037815126050420166 \t0.84375 \t0.2404342\n",
      "0.037868162692847124 \t0.84375 \t0.24045037\n",
      "0.037921348314606744 \t0.84375 \t0.24045068\n",
      "0.0379746835443038 \t0.84375 \t0.24049634\n",
      "0.038028169014084505 \t0.84375 \t0.24052247\n",
      "0.0380818053596615 \t0.84375 \t0.24053203\n",
      "0.038135593220338986 \t0.84375 \t0.24055241\n",
      "0.03818953323903819 \t0.84375 \t0.24056135\n",
      "0.03824362606232295 \t0.84375 \t0.24056298\n",
      "0.03829787234042553 \t0.84375 \t0.2405733\n",
      "0.03835227272727273 \t0.84375 \t0.2406161\n",
      "0.03840682788051209 \t0.84375 \t0.24062453\n",
      "0.038461538461538464 \t0.84375 \t0.24065341\n",
      "0.03851640513552068 \t0.84375 \t0.2407085\n",
      "0.03857142857142857 \t0.84375 \t0.24071717\n",
      "0.03862660944206009 \t0.84375 \t0.24072412\n",
      "0.03868194842406877 \t0.84375 \t0.24076863\n",
      "0.03873744619799139 \t0.84375 \t0.24081638\n",
      "0.03879310344827586 \t0.84375 \t0.24083926\n",
      "0.03884892086330935 \t0.84375 \t0.24085496\n",
      "0.03890489913544669 \t0.84375 \t0.2408646\n",
      "0.03896103896103896 \t0.84375 \t0.24093115\n",
      "0.03901734104046243 \t0.84375 \t0.24094631\n",
      "0.03907380607814761 \t0.84375 \t0.24095033\n",
      "0.0391304347826087 \t0.84375 \t0.24095201\n",
      "0.03918722786647315 \t0.84375 \t0.24095449\n",
      "0.03924418604651163 \t0.84375 \t0.24101022\n",
      "0.039301310043668124 \t0.84375 \t0.24102788\n",
      "0.03935860058309038 \t0.84375 \t0.24107976\n",
      "0.03941605839416058 \t0.84375 \t0.24108805\n",
      "0.039473684210526314 \t0.84375 \t0.24109015\n",
      "0.03953147877013177 \t0.84375 \t0.24110416\n",
      "0.039589442815249266 \t0.84375 \t0.24114844\n",
      "0.039647577092511016 \t0.84375 \t0.24116667\n",
      "0.039705882352941174 \t0.84375 \t0.24120818\n",
      "0.039764359351988215 \t0.84375 \t0.24129108\n",
      "0.03982300884955752 \t0.84375 \t0.24129623\n",
      "0.03988183161004431 \t0.84375 \t0.24130069\n",
      "0.03994082840236687 \t0.84375 \t0.24133943\n",
      "0.04 \t0.84375 \t0.2413699\n",
      "0.040059347181008904 \t0.84375 \t0.24142267\n",
      "0.04011887072808321 \t0.84375 \t0.24150148\n",
      "0.04017857142857143 \t0.84375 \t0.24153446\n",
      "0.040238450074515646 \t0.84375 \t0.24153708\n",
      "0.04029850746268657 \t0.84375 \t0.24157196\n",
      "0.04035874439461883 \t0.84375 \t0.2415933\n",
      "0.04047976011994003 \t0.84375 \t0.24170674\n",
      "0.04054054054054054 \t0.84375 \t0.24173424\n",
      "0.0406015037593985 \t0.84375 \t0.24194412\n",
      "0.04066265060240964 \t0.84375 \t0.2419933\n",
      "0.04072398190045249 \t0.84375 \t0.2420531\n",
      "0.04078549848942598 \t0.84375 \t0.24207918\n",
      "0.04084720121028744 \t0.84375 \t0.24211481\n",
      "0.04090909090909091 \t0.84375 \t0.24222744\n",
      "0.0409711684370258 \t0.84375 \t0.24235241\n",
      "0.041033434650455926 \t0.84375 \t0.24240197\n",
      "0.0410958904109589 \t0.84375 \t0.24241114\n",
      "0.039634146341463415 \t0.8125 \t0.2425632\n",
      "0.03969465648854962 \t0.8125 \t0.24267432\n",
      "0.039755351681957186 \t0.8125 \t0.24280222\n",
      "0.039816232771822356 \t0.8125 \t0.24293944\n",
      "0.03987730061349693 \t0.8125 \t0.24297814\n",
      "0.039938556067588324 \t0.8125 \t0.2429972\n",
      "0.04 \t0.8125 \t0.24305272\n",
      "0.040061633281972264 \t0.8125 \t0.24313436\n",
      "0.040123456790123455 \t0.8125 \t0.24317273\n",
      "0.0401854714064915 \t0.8125 \t0.24323037\n",
      "0.04024767801857585 \t0.8125 \t0.24338987\n",
      "0.040310077519379844 \t0.8125 \t0.24342884\n",
      "0.040372670807453416 \t0.8125 \t0.24344724\n",
      "0.04043545878693624 \t0.8125 \t0.24346586\n",
      "0.040498442367601244 \t0.8125 \t0.24360956\n",
      "0.0405616224648986 \t0.8125 \t0.24364088\n",
      "0.040625 \t0.8125 \t0.24364302\n",
      "0.0406885758998435 \t0.8125 \t0.24376243\n",
      "0.04075235109717868 \t0.8125 \t0.24378878\n",
      "0.04081632653061224 \t0.8125 \t0.24385722\n",
      "0.040880503144654086 \t0.8125 \t0.24388327\n",
      "0.04094488188976378 \t0.8125 \t0.24396159\n",
      "0.04100946372239748 \t0.8125 \t0.24397759\n",
      "0.04107424960505529 \t0.8125 \t0.24400641\n",
      "0.04113924050632911 \t0.8125 \t0.24404381\n",
      "0.04120443740095087 \t0.8125 \t0.24411127\n",
      "0.04126984126984127 \t0.8125 \t0.24431714\n",
      "0.04133545310015898 \t0.8125 \t0.24434108\n",
      "0.041401273885350316 \t0.8125 \t0.24455476\n",
      "0.04146730462519936 \t0.8125 \t0.24466428\n",
      "0.04153354632587859 \t0.8125 \t0.24471782\n",
      "0.0416 \t0.8125 \t0.24491455\n",
      "0.041666666666666664 \t0.8125 \t0.24492846\n",
      "0.04173354735152488 \t0.8125 \t0.24496034\n",
      "0.04180064308681672 \t0.8125 \t0.24500342\n",
      "0.04186795491143317 \t0.8125 \t0.24508972\n",
      "0.041935483870967745 \t0.8125 \t0.24512666\n",
      "0.0420032310177706 \t0.8125 \t0.24519084\n",
      "0.042071197411003236 \t0.8125 \t0.24534711\n",
      "0.04213938411669368 \t0.8125 \t0.24538209\n",
      "0.04220779220779221 \t0.8125 \t0.24540815\n",
      "0.04227642276422764 \t0.8125 \t0.24544236\n",
      "0.04234527687296417 \t0.8125 \t0.24560691\n",
      "0.04241435562805873 \t0.8125 \t0.24574101\n",
      "0.042483660130718956 \t0.8125 \t0.24585986\n",
      "0.0425531914893617 \t0.8125 \t0.24594617\n",
      "0.04262295081967213 \t0.8125 \t0.24596396\n",
      "0.042692939244663386 \t0.8125 \t0.24606374\n",
      "0.04276315789473684 \t0.8125 \t0.24607013\n",
      "0.042833607907743 \t0.8125 \t0.24611235\n",
      "0.0429042904290429 \t0.8125 \t0.24614337\n",
      "0.04297520661157025 \t0.8125 \t0.24615061\n",
      "0.04304635761589404 \t0.8125 \t0.24624678\n",
      "0.04311774461028192 \t0.8125 \t0.24641456\n",
      "0.04318936877076412 \t0.8125 \t0.24642804\n",
      "0.04326123128119801 \t0.8125 \t0.24649617\n",
      "0.043333333333333335 \t0.8125 \t0.24650328\n",
      "0.04340567612687813 \t0.8125 \t0.2466127\n",
      "0.043478260869565216 \t0.8125 \t0.24661574\n",
      "0.04355108877721943 \t0.8125 \t0.24666342\n",
      "0.0436241610738255 \t0.8125 \t0.24667418\n",
      "0.043697478991596636 \t0.8125 \t0.24668229\n",
      "0.04377104377104377 \t0.8125 \t0.24676184\n",
      "0.04384485666104553 \t0.8125 \t0.2467713\n",
      "0.04391891891891892 \t0.8125 \t0.24677213\n",
      "0.043993231810490696 \t0.8125 \t0.24678537\n",
      "0.0423728813559322 \t0.78125 \t0.2467858\n",
      "0.042444821731748725 \t0.78125 \t0.24678707\n",
      "0.04251700680272109 \t0.78125 \t0.24682109\n",
      "0.04258943781942078 \t0.78125 \t0.24685179\n",
      "0.042662116040955635 \t0.78125 \t0.2468678\n",
      "0.042735042735042736 \t0.78125 \t0.24690272\n",
      "0.04280821917808219 \t0.78125 \t0.24693455\n",
      "0.04288164665523156 \t0.78125 \t0.246939\n",
      "0.0429553264604811 \t0.78125 \t0.24694511\n",
      "0.043029259896729774 \t0.78125 \t0.24694513\n",
      "0.04310344827586207 \t0.78125 \t0.24697565\n",
      "0.04317789291882556 \t0.78125 \t0.24698749\n",
      "0.04325259515570934 \t0.78125 \t0.24699007\n",
      "0.043327556325823226 \t0.78125 \t0.2470212\n",
      "0.043402777777777776 \t0.78125 \t0.24704774\n",
      "0.043478260869565216 \t0.78125 \t0.2470502\n",
      "0.04355400696864112 \t0.78125 \t0.24711949\n",
      "0.04363001745200698 \t0.78125 \t0.24712129\n",
      "0.043706293706293704 \t0.78125 \t0.24716768\n",
      "0.043782837127845885 \t0.78125 \t0.24716811\n",
      "0.043859649122807015 \t0.78125 \t0.24727842\n",
      "0.043936731107205626 \t0.78125 \t0.2473458\n",
      "0.04401408450704225 \t0.78125 \t0.24735764\n",
      "0.04409171075837742 \t0.78125 \t0.24737222\n",
      "0.044169611307420496 \t0.78125 \t0.24746779\n",
      "0.04424778761061947 \t0.78125 \t0.24755841\n",
      "0.044326241134751775 \t0.78125 \t0.24758393\n",
      "0.04440497335701599 \t0.78125 \t0.24761306\n",
      "0.04448398576512456 \t0.78125 \t0.2476172\n",
      "0.044563279857397504 \t0.78125 \t0.24769342\n",
      "0.044722719141323794 \t0.78125 \t0.2477071\n",
      "0.04488330341113106 \t0.78125 \t0.24770984\n",
      "0.044964028776978415 \t0.78125 \t0.2477254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04504504504504504 \t0.78125 \t0.24774924\n",
      "0.04512635379061372 \t0.78125 \t0.24775149\n",
      "0.045207956600361664 \t0.78125 \t0.2477746\n",
      "0.04528985507246377 \t0.78125 \t0.24788144\n",
      "0.045372050816696916 \t0.78125 \t0.24801053\n",
      "0.045454545454545456 \t0.78125 \t0.24805725\n",
      "0.04371584699453552 \t0.75 \t0.24808764\n",
      "0.043795620437956206 \t0.75 \t0.2481318\n",
      "0.043875685557586835 \t0.75 \t0.24826422\n",
      "0.04395604395604396 \t0.75 \t0.24830414\n",
      "0.04436229205175601 \t0.75 \t0.24830711\n",
      "0.044444444444444446 \t0.75 \t0.2483499\n",
      "0.04452690166975881 \t0.75 \t0.24835014\n",
      "0.04460966542750929 \t0.75 \t0.24835603\n",
      "0.0446927374301676 \t0.75 \t0.24836175\n",
      "0.044859813084112146 \t0.75 \t0.2483618\n",
      "0.0450281425891182 \t0.75 \t0.24838014\n",
      "0.045112781954887216 \t0.75 \t0.2484523\n",
      "0.04519774011299435 \t0.75 \t0.24851088\n",
      "0.045283018867924525 \t0.75 \t0.24854268\n",
      "0.045368620037807186 \t0.75 \t0.24856421\n",
      "0.045454545454545456 \t0.75 \t0.24863042\n",
      "0.04554079696394687 \t0.75 \t0.24863799\n",
      "0.045627376425855515 \t0.75 \t0.2486407\n",
      "0.045714285714285714 \t0.75 \t0.24871454\n",
      "0.04580152671755725 \t0.75 \t0.24871702\n",
      "0.045889101338432124 \t0.75 \t0.2487203\n",
      "0.04597701149425287 \t0.75 \t0.24872084\n",
      "0.046242774566473986 \t0.75 \t0.24874273\n",
      "0.04669260700389105 \t0.75 \t0.24876429\n",
      "0.04678362573099415 \t0.75 \t0.24877746\n",
      "0.047337278106508875 \t0.75 \t0.2487849\n",
      "0.04743083003952569 \t0.75 \t0.24878865\n",
      "0.047524752475247525 \t0.75 \t0.2487949\n",
      "0.04790419161676647 \t0.75 \t0.24879491\n",
      "0.0482897384305835 \t0.75 \t0.24883804\n",
      "0.04838709677419355 \t0.75 \t0.24884741\n",
      "0.048484848484848485 \t0.75 \t0.24893093\n",
      "0.048582995951417005 \t0.75 \t0.24898744\n",
      "0.0486815415821501 \t0.75 \t0.24900714\n",
      "0.04878048780487805 \t0.75 \t0.2490911\n",
      "0.048879837067209775 \t0.75 \t0.24912646\n",
      "0.049079754601226995 \t0.75 \t0.24914356\n",
      "0.04918032786885246 \t0.75 \t0.24915962\n",
      "0.049281314168377825 \t0.75 \t0.24919531\n",
      "0.04938271604938271 \t0.75 \t0.24920955\n",
      "0.049689440993788817 \t0.75 \t0.24921037\n",
      "0.04979253112033195 \t0.75 \t0.24921589\n",
      "0.0498960498960499 \t0.75 \t0.24921739\n",
      "0.05 \t0.75 \t0.2492229\n",
      "0.05010438413361169 \t0.75 \t0.2492812\n",
      "0.0502092050209205 \t0.75 \t0.24928401\n",
      "0.050314465408805034 \t0.75 \t0.24928765\n",
      "0.05042016806722689 \t0.75 \t0.24928856\n",
      "0.05052631578947368 \t0.75 \t0.24928881\n",
      "0.05084745762711865 \t0.75 \t0.24929069\n",
      "0.050955414012738856 \t0.75 \t0.2493017\n",
      "0.05106382978723404 \t0.75 \t0.24930756\n",
      "0.0511727078891258 \t0.75 \t0.24930792\n",
      "0.05128205128205128 \t0.75 \t0.2493082\n",
      "0.05139186295503212 \t0.75 \t0.24931566\n",
      "0.05150214592274678 \t0.75 \t0.24932711\n",
      "0.05161290322580645 \t0.75 \t0.24936973\n",
      "0.05172413793103448 \t0.75 \t0.2493702\n",
      "0.05183585313174946 \t0.75 \t0.2493821\n",
      "0.05217391304347826 \t0.75 \t0.24938214\n",
      "0.0525164113785558 \t0.75 \t0.24942002\n",
      "0.05263157894736842 \t0.75 \t0.24942756\n",
      "0.05274725274725275 \t0.75 \t0.24943094\n",
      "0.05286343612334802 \t0.75 \t0.24943201\n",
      "0.052980132450331126 \t0.75 \t0.24943273\n",
      "0.05309734513274336 \t0.75 \t0.24943398\n",
      "0.05321507760532151 \t0.75 \t0.24943416\n",
      "0.05333333333333334 \t0.75 \t0.24944267\n",
      "0.053811659192825115 \t0.75 \t0.24944688\n",
      "0.05393258426966292 \t0.75 \t0.24951313\n",
      "0.05405405405405406 \t0.75 \t0.24953157\n",
      "0.05417607223476298 \t0.75 \t0.24953346\n",
      "0.05429864253393665 \t0.75 \t0.24956553\n",
      "0.05442176870748299 \t0.75 \t0.2495667\n",
      "0.05466970387243736 \t0.75 \t0.24958172\n",
      "0.0547945205479452 \t0.75 \t0.24960412\n",
      "0.05555555555555555 \t0.75 \t0.24961254\n",
      "0.05568445475638051 \t0.75 \t0.24964449\n",
      "0.05581395348837209 \t0.75 \t0.24964873\n",
      "0.055944055944055944 \t0.75 \t0.24966437\n",
      "0.056074766355140186 \t0.75 \t0.2496773\n",
      "0.05620608899297424 \t0.75 \t0.24968086\n",
      "0.056338028169014086 \t0.75 \t0.24968109\n",
      "0.05647058823529412 \t0.75 \t0.24968164\n",
      "0.05660377358490566 \t0.75 \t0.24968176\n",
      "0.05673758865248227 \t0.75 \t0.2496922\n",
      "0.05687203791469194 \t0.75 \t0.24969456\n",
      "0.057007125890736345 \t0.75 \t0.24969703\n",
      "0.05714285714285714 \t0.75 \t0.24971044\n",
      "0.057279236276849645 \t0.75 \t0.24971046\n",
      "0.05755395683453238 \t0.75 \t0.24971497\n",
      "0.057971014492753624 \t0.75 \t0.24973294\n",
      "0.05825242718446602 \t0.75 \t0.24977407\n",
      "0.058394160583941604 \t0.75 \t0.24979596\n",
      "0.05853658536585366 \t0.75 \t0.24980949\n",
      "0.05867970660146699 \t0.75 \t0.24981023\n",
      "0.058823529411764705 \t0.75 \t0.24982707\n",
      "0.05955334987593052 \t0.75 \t0.24985062\n",
      "0.05970149253731343 \t0.75 \t0.2498764\n",
      "0.059850374064837904 \t0.75 \t0.24998985\n",
      "0.06015037593984962 \t0.75 \t0.24999727\n",
      "0.06030150753768844 \t0.75 \t0.2499997\n",
      "0.060453400503778336 \t0.75 \t0.25006297\n",
      "0.06060606060606061 \t0.75 \t0.25018752\n",
      "0.060759493670886074 \t0.75 \t0.25020805\n",
      "0.06091370558375635 \t0.75 \t0.25021642\n",
      "0.061068702290076333 \t0.75 \t0.25023\n",
      "0.061224489795918366 \t0.75 \t0.25023773\n",
      "0.06201550387596899 \t0.75 \t0.2502379\n",
      "0.06217616580310881 \t0.75 \t0.25024444\n",
      "0.06233766233766234 \t0.75 \t0.2502592\n",
      "0.0625 \t0.75 \t0.25026762\n",
      "0.06266318537859007 \t0.75 \t0.25033504\n",
      "0.06282722513089005 \t0.75 \t0.25033766\n",
      "0.06299212598425197 \t0.75 \t0.250339\n",
      "0.06315789473684211 \t0.75 \t0.25037357\n",
      "0.0633245382585752 \t0.75 \t0.250387\n",
      "0.06349206349206349 \t0.75 \t0.2503895\n",
      "0.0636604774535809 \t0.75 \t0.25043052\n",
      "0.061170212765957445 \t0.71875 \t0.250432\n",
      "0.06133333333333333 \t0.71875 \t0.25051674\n",
      "0.06149732620320856 \t0.71875 \t0.25063688\n",
      "0.06166219839142091 \t0.71875 \t0.25064257\n",
      "0.06182795698924731 \t0.71875 \t0.25065213\n",
      "0.06199460916442048 \t0.71875 \t0.2506603\n",
      "0.062162162162162166 \t0.71875 \t0.2507233\n",
      "0.06233062330623306 \t0.71875 \t0.2507535\n",
      "0.0625 \t0.71875 \t0.2508088\n",
      "0.06267029972752043 \t0.71875 \t0.2508108\n",
      "0.06284153005464481 \t0.71875 \t0.2508316\n",
      "0.06301369863013699 \t0.71875 \t0.25090125\n",
      "0.06318681318681318 \t0.71875 \t0.25090623\n",
      "0.06336088154269973 \t0.71875 \t0.2509098\n",
      "0.0647887323943662 \t0.71875 \t0.25091138\n",
      "0.06497175141242938 \t0.71875 \t0.25093856\n",
      "0.06515580736543909 \t0.71875 \t0.25094202\n",
      "0.06534090909090909 \t0.71875 \t0.25094205\n",
      "0.06552706552706553 \t0.71875 \t0.25094855\n",
      "0.06571428571428571 \t0.71875 \t0.25095087\n",
      "0.0659025787965616 \t0.71875 \t0.2509735\n",
      "0.06609195402298851 \t0.71875 \t0.25097996\n",
      "0.06628242074927954 \t0.71875 \t0.25102538\n",
      "0.06647398843930635 \t0.71875 \t0.2510838\n",
      "0.06666666666666667 \t0.71875 \t0.2511507\n",
      "0.06686046511627906 \t0.71875 \t0.25115183\n",
      "0.06705539358600583 \t0.71875 \t0.2511574\n",
      "0.06725146198830409 \t0.71875 \t0.2511579\n",
      "0.06744868035190615 \t0.71875 \t0.25123826\n",
      "0.06764705882352941 \t0.71875 \t0.25127986\n",
      "0.06784660766961652 \t0.71875 \t0.2514339\n",
      "0.06804733727810651 \t0.71875 \t0.25145015\n",
      "0.06528189910979229 \t0.6875 \t0.25165775\n",
      "0.06547619047619048 \t0.6875 \t0.251698\n",
      "0.06567164179104477 \t0.6875 \t0.25176316\n",
      "0.0658682634730539 \t0.6875 \t0.25200966\n",
      "0.06606606606606606 \t0.6875 \t0.25201836\n",
      "0.06626506024096386 \t0.6875 \t0.25204653\n",
      "0.06646525679758308 \t0.6875 \t0.25228807\n",
      "0.06666666666666667 \t0.6875 \t0.2525775\n",
      "0.0668693009118541 \t0.6875 \t0.25276738\n",
      "0.06707317073170732 \t0.6875 \t0.2527828\n",
      "0.0672782874617737 \t0.6875 \t0.252872\n",
      "0.06748466257668712 \t0.6875 \t0.2528771\n",
      "0.06769230769230769 \t0.6875 \t0.25298518\n",
      "0.06790123456790123 \t0.6875 \t0.2529946\n",
      "0.06811145510835913 \t0.6875 \t0.2530129\n",
      "0.06832298136645963 \t0.6875 \t0.25306273\n",
      "0.06853582554517133 \t0.6875 \t0.2531226\n",
      "0.06875 \t0.6875 \t0.25312793\n",
      "0.06896551724137931 \t0.6875 \t0.25320232\n",
      "0.06918238993710692 \t0.6875 \t0.25321704\n",
      "0.0694006309148265 \t0.6875 \t0.25329682\n",
      "0.06962025316455696 \t0.6875 \t0.2533895\n",
      "0.06984126984126984 \t0.6875 \t0.25341222\n",
      "0.07006369426751592 \t0.6875 \t0.2537631\n",
      "0.07028753993610223 \t0.6875 \t0.25421306\n",
      "0.07051282051282051 \t0.6875 \t0.25443283\n",
      "0.0707395498392283 \t0.6875 \t0.25445804\n",
      "0.07096774193548387 \t0.6875 \t0.25490472\n",
      "0.06796116504854369 \t0.65625 \t0.2550756\n",
      "0.06818181818181818 \t0.65625 \t0.25508913\n",
      "0.06840390879478828 \t0.65625 \t0.25553623\n",
      "0.06862745098039216 \t0.65625 \t0.25568682\n",
      "0.06885245901639345 \t0.65625 \t0.25570443\n",
      "0.06907894736842106 \t0.65625 \t0.25579077\n",
      "0.06930693069306931 \t0.65625 \t0.25582117\n",
      "0.0695364238410596 \t0.65625 \t0.25583676\n",
      "0.06976744186046512 \t0.65625 \t0.25594872\n",
      "0.07 \t0.65625 \t0.25594893\n",
      "0.07023411371237458 \t0.65625 \t0.25599498\n",
      "0.07046979865771812 \t0.65625 \t0.2560142\n",
      "0.0707070707070707 \t0.65625 \t0.25612447\n",
      "0.07094594594594594 \t0.65625 \t0.25617868\n",
      "0.0711864406779661 \t0.65625 \t0.25618234\n",
      "0.07142857142857142 \t0.65625 \t0.2562132\n",
      "0.07167235494880546 \t0.65625 \t0.25624776\n",
      "0.07191780821917808 \t0.65625 \t0.2562487\n",
      "0.07216494845360824 \t0.65625 \t0.25636786\n",
      "0.07241379310344828 \t0.65625 \t0.2566037\n",
      "0.0726643598615917 \t0.65625 \t0.25669393\n",
      "0.07291666666666667 \t0.65625 \t0.2567086\n",
      "0.07317073170731707 \t0.65625 \t0.25672188\n",
      "0.07342657342657342 \t0.65625 \t0.2567437\n",
      "0.07368421052631578 \t0.65625 \t0.25674388\n",
      "0.07042253521126761 \t0.625 \t0.25677508\n",
      "0.0706713780918728 \t0.625 \t0.2568293\n",
      "0.07092198581560284 \t0.625 \t0.25683537\n",
      "0.0711743772241993 \t0.625 \t0.25683808\n",
      "0.07142857142857142 \t0.625 \t0.25686613\n",
      "0.07168458781362007 \t0.625 \t0.25690275\n",
      "0.07194244604316546 \t0.625 \t0.2569148\n",
      "0.07220216606498195 \t0.625 \t0.25692394\n",
      "0.07246376811594203 \t0.625 \t0.25694185\n",
      "0.07272727272727272 \t0.625 \t0.25695327\n",
      "0.072992700729927 \t0.625 \t0.25699022\n",
      "0.07326007326007326 \t0.625 \t0.25701827\n",
      "0.07352941176470588 \t0.625 \t0.2572356\n",
      "0.07380073800738007 \t0.625 \t0.25723803\n",
      "0.07407407407407407 \t0.625 \t0.25725576\n",
      "0.07434944237918216 \t0.625 \t0.25726143\n",
      "0.07462686567164178 \t0.625 \t0.2573383\n",
      "0.0749063670411985 \t0.625 \t0.25735465\n",
      "0.07518796992481203 \t0.625 \t0.25741595\n",
      "0.07547169811320754 \t0.625 \t0.25741947\n",
      "0.07575757575757576 \t0.625 \t0.25743407\n",
      "0.07604562737642585 \t0.625 \t0.25746736\n",
      "0.07633587786259542 \t0.625 \t0.2575116\n",
      "0.07662835249042145 \t0.625 \t0.25751925\n",
      "0.07722007722007722 \t0.625 \t0.2575395\n",
      "0.07751937984496124 \t0.625 \t0.25755593\n",
      "0.07782101167315175 \t0.625 \t0.25755605\n",
      "0.078125 \t0.625 \t0.2575616\n",
      "0.0784313725490196 \t0.625 \t0.2575673\n",
      "0.07874015748031496 \t0.625 \t0.25761342\n",
      "0.07905138339920949 \t0.625 \t0.25772738\n",
      "0.07936507936507936 \t0.625 \t0.25774837\n",
      "0.0796812749003984 \t0.625 \t0.2577962\n",
      "0.08 \t0.625 \t0.25781336\n",
      "0.08032128514056225 \t0.625 \t0.25782648\n",
      "0.08064516129032258 \t0.625 \t0.25791413\n",
      "0.08097165991902834 \t0.625 \t0.25813386\n",
      "0.08130081300813008 \t0.625 \t0.25824314\n",
      "0.08163265306122448 \t0.625 \t0.2583017\n",
      "0.08196721311475409 \t0.625 \t0.25831956\n",
      "0.07818930041152264 \t0.59375 \t0.25852895\n",
      "0.07851239669421488 \t0.59375 \t0.25879902\n",
      "0.07883817427385892 \t0.59375 \t0.25885996\n",
      "0.07916666666666666 \t0.59375 \t0.25890774\n",
      "0.0794979079497908 \t0.59375 \t0.2589096\n",
      "0.07983193277310924 \t0.59375 \t0.2589144\n",
      "0.08050847457627118 \t0.59375 \t0.25892988\n",
      "0.0811965811965812 \t0.59375 \t0.25893638\n",
      "0.0815450643776824 \t0.59375 \t0.2589395\n",
      "0.08189655172413793 \t0.59375 \t0.25895292\n",
      "0.08260869565217391 \t0.59375 \t0.25896737\n",
      "0.08370044052863436 \t0.59375 \t0.25898165\n",
      "0.084070796460177 \t0.59375 \t0.2590773\n",
      "0.08444444444444445 \t0.59375 \t0.25909293\n",
      "0.08482142857142858 \t0.59375 \t0.2591053\n",
      "0.08520179372197309 \t0.59375 \t0.25912175\n",
      "0.08558558558558559 \t0.59375 \t0.25917622\n",
      "0.08597285067873303 \t0.59375 \t0.25921577\n",
      "0.08796296296296297 \t0.59375 \t0.25921637\n",
      "0.0892018779342723 \t0.59375 \t0.25921693\n",
      "0.09004739336492891 \t0.59375 \t0.25923812\n",
      "0.09047619047619047 \t0.59375 \t0.2593119\n",
      "0.09090909090909091 \t0.59375 \t0.25932363\n",
      "0.09178743961352658 \t0.59375 \t0.25934014\n",
      "0.09547738693467336 \t0.59375 \t0.25935006\n",
      "0.09595959595959595 \t0.59375 \t0.25946125\n",
      "0.09644670050761421 \t0.59375 \t0.25955322\n",
      "0.09693877551020408 \t0.59375 \t0.25956807\n",
      "0.1 \t0.59375 \t0.25957447\n",
      "0.10052910052910052 \t0.59375 \t0.25957644\n",
      "0.10215053763440861 \t0.59375 \t0.25971234\n",
      "0.10270270270270271 \t0.59375 \t0.25978556\n",
      "0.10326086956521739 \t0.59375 \t0.25982234\n",
      "0.10382513661202186 \t0.59375 \t0.26035324\n",
      "0.1043956043956044 \t0.59375 \t0.2604112\n",
      "0.10497237569060773 \t0.59375 \t0.26062843\n",
      "0.10614525139664804 \t0.59375 \t0.2606432\n",
      "0.10674157303370786 \t0.59375 \t0.26070955\n",
      "0.10734463276836158 \t0.59375 \t0.26073694\n",
      "0.10795454545454546 \t0.59375 \t0.2607769\n",
      "0.10857142857142857 \t0.59375 \t0.26087287\n",
      "0.10982658959537572 \t0.59375 \t0.26090646\n",
      "0.11046511627906977 \t0.59375 \t0.2609175\n",
      "0.1111111111111111 \t0.59375 \t0.26101294\n",
      "0.11176470588235295 \t0.59375 \t0.2610257\n",
      "0.11728395061728394 \t0.59375 \t0.2613326\n",
      "0.11801242236024845 \t0.59375 \t0.26135597\n",
      "0.11875 \t0.59375 \t0.26147008\n",
      "0.11949685534591195 \t0.59375 \t0.2614835\n",
      "0.12025316455696203 \t0.59375 \t0.26148367\n",
      "0.12258064516129032 \t0.59375 \t0.2614974\n",
      "0.1292517006802721 \t0.59375 \t0.2616753\n",
      "0.13013698630136986 \t0.59375 \t0.26169306\n",
      "0.1310344827586207 \t0.59375 \t0.26170078\n",
      "0.13380281690140844 \t0.59375 \t0.26170334\n",
      "0.1347517730496454 \t0.59375 \t0.26172465\n",
      "0.1357142857142857 \t0.59375 \t0.2617496\n",
      "0.1366906474820144 \t0.59375 \t0.26175088\n",
      "0.1386861313868613 \t0.59375 \t0.2617554\n",
      "0.1417910447761194 \t0.59375 \t0.26176795\n",
      "0.14285714285714285 \t0.59375 \t0.26177067\n",
      "0.14393939393939395 \t0.59375 \t0.26203606\n",
      "0.1450381679389313 \t0.59375 \t0.26205155\n",
      "0.14615384615384616 \t0.59375 \t0.26343158\n",
      "0.14728682170542637 \t0.59375 \t0.2635825\n",
      "0.1484375 \t0.59375 \t0.2636309\n",
      "0.14960629921259844 \t0.59375 \t0.26376665\n",
      "0.15079365079365079 \t0.59375 \t0.26501438\n",
      "0.152 \t0.59375 \t0.26523075\n",
      "0.1532258064516129 \t0.59375 \t0.2655769\n",
      "0.15447154471544716 \t0.59375 \t0.26570463\n",
      "0.1557377049180328 \t0.59375 \t0.26644364\n",
      "0.1487603305785124 \t0.5625 \t0.26655668\n",
      "0.15 \t0.5625 \t0.2666025\n",
      "0.14285714285714285 \t0.53125 \t0.2667989\n",
      "0.13559322033898305 \t0.5 \t0.2668885\n",
      "0.13675213675213677 \t0.5 \t0.26690245\n",
      "0.12931034482758622 \t0.46875 \t0.26700473\n",
      "0.13043478260869565 \t0.46875 \t0.26702264\n",
      "0.13157894736842105 \t0.46875 \t0.26731357\n",
      "0.13274336283185842 \t0.46875 \t0.267616\n",
      "0.13392857142857142 \t0.46875 \t0.26769966\n",
      "0.13513513513513514 \t0.46875 \t0.26801008\n",
      "0.13636363636363635 \t0.46875 \t0.26805535\n",
      "0.13761467889908258 \t0.46875 \t0.26843455\n",
      "0.1388888888888889 \t0.46875 \t0.26860687\n",
      "0.14018691588785046 \t0.46875 \t0.26870933\n",
      "0.14150943396226415 \t0.46875 \t0.2687884\n",
      "0.14285714285714285 \t0.46875 \t0.26924023\n",
      "0.14423076923076922 \t0.46875 \t0.2695223\n",
      "0.14563106796116504 \t0.46875 \t0.26988667\n",
      "0.14705882352941177 \t0.46875 \t0.27036044\n",
      "0.1485148514851485 \t0.46875 \t0.2707429\n",
      "0.15 \t0.46875 \t0.27075353\n",
      "0.15151515151515152 \t0.46875 \t0.27082333\n",
      "0.15306122448979592 \t0.46875 \t0.2711423\n",
      "0.15463917525773196 \t0.46875 \t0.27172932\n",
      "0.15625 \t0.46875 \t0.272657\n",
      "0.14736842105263157 \t0.4375 \t0.27283633\n",
      "0.14893617021276595 \t0.4375 \t0.2737498\n",
      "0.15053763440860216 \t0.4375 \t0.2739409\n",
      "0.15217391304347827 \t0.4375 \t0.27468625\n",
      "0.15384615384615385 \t0.4375 \t0.27528226\n",
      "0.15555555555555556 \t0.4375 \t0.27595186\n",
      "0.15730337078651685 \t0.4375 \t0.27598527\n",
      "0.14772727272727273 \t0.40625 \t0.2789191\n",
      "0.14942528735632185 \t0.40625 \t0.27911666\n",
      "0.1511627906976744 \t0.40625 \t0.27926844\n",
      "0.15294117647058825 \t0.40625 \t0.28074747\n",
      "0.15476190476190477 \t0.40625 \t0.28310078\n",
      "0.1566265060240964 \t0.40625 \t0.2831761\n",
      "0.15853658536585366 \t0.40625 \t0.2842667\n",
      "0.16049382716049382 \t0.40625 \t0.28430304\n",
      "0.1625 \t0.40625 \t0.28463864\n",
      "0.16455696202531644 \t0.40625 \t0.28559637\n",
      "0.16666666666666666 \t0.40625 \t0.28582188\n",
      "0.16883116883116883 \t0.40625 \t0.2888212\n",
      "0.17105263157894737 \t0.40625 \t0.28903866\n",
      "0.16 \t0.375 \t0.28970248\n",
      "0.16216216216216217 \t0.375 \t0.28973046\n",
      "0.1506849315068493 \t0.34375 \t0.29011548\n",
      "0.1527777777777778 \t0.34375 \t0.29039198\n",
      "0.14084507042253522 \t0.3125 \t0.29131952\n",
      "0.12857142857142856 \t0.28125 \t0.29161316\n",
      "0.13043478260869565 \t0.28125 \t0.2942428\n",
      "0.11764705882352941 \t0.25 \t0.2947894\n",
      "0.11940298507462686 \t0.25 \t0.29768646\n",
      "0.10606060606060606 \t0.21875 \t0.29883525\n",
      "0.1076923076923077 \t0.21875 \t0.29895633\n",
      "0.109375 \t0.21875 \t0.30088788\n",
      "0.1111111111111111 \t0.21875 \t0.3021279\n",
      "0.11290322580645161 \t0.21875 \t0.30258003\n",
      "0.09836065573770492 \t0.1875 \t0.30408242\n",
      "0.1 \t0.1875 \t0.3049949\n",
      "0.1016949152542373 \t0.1875 \t0.3066022\n",
      "0.08620689655172414 \t0.15625 \t0.30742574\n",
      "0.07017543859649122 \t0.125 \t0.31280175\n",
      "0.07142857142857142 \t0.125 \t0.31433132\n",
      "0.07272727272727272 \t0.125 \t0.3145338\n",
      "0.07407407407407407 \t0.125 \t0.3170584\n",
      "0.07547169811320754 \t0.125 \t0.31865335\n",
      "0.07692307692307693 \t0.125 \t0.3198245\n",
      "0.0784313725490196 \t0.125 \t0.32026383\n",
      "0.08 \t0.125 \t0.3223518\n",
      "0.08163265306122448 \t0.125 \t0.32458606\n",
      "0.08333333333333333 \t0.125 \t0.32755134\n",
      "0.0851063829787234 \t0.125 \t0.32972312\n",
      "0.06521739130434782 \t0.09375 \t0.33047235\n",
      "0.06666666666666667 \t0.09375 \t0.333231\n",
      "0.06818181818181818 \t0.09375 \t0.33400384\n",
      "0.06976744186046512 \t0.09375 \t0.3360316\n",
      "0.07142857142857142 \t0.09375 \t0.33965456\n",
      "0.07317073170731707 \t0.09375 \t0.34265208\n",
      "0.05 \t0.0625 \t0.34346506\n",
      "0.05128205128205128 \t0.0625 \t0.34397143\n",
      "0.05263157894736842 \t0.0625 \t0.34700054\n",
      "0.05405405405405406 \t0.0625 \t0.34811127\n",
      "0.05555555555555555 \t0.0625 \t0.34829473\n",
      "0.05714285714285714 \t0.0625 \t0.34962234\n",
      "0.058823529411764705 \t0.0625 \t0.35253587\n",
      "0.06060606060606061 \t0.0625 \t0.35416886\n",
      "0.0625 \t0.0625 \t0.35566232\n",
      "0.06451612903225806 \t0.0625 \t0.3563823\n",
      "0.06666666666666667 \t0.0625 \t0.3565739\n",
      "0.06896551724137931 \t0.0625 \t0.35774428\n",
      "0.07142857142857142 \t0.0625 \t0.35903388\n",
      "0.07407407407407407 \t0.0625 \t0.36002237\n",
      "0.038461538461538464 \t0.03125 \t0.36037797\n",
      "0.04 \t0.03125 \t0.3604876\n",
      "0.041666666666666664 \t0.03125 \t0.36079252\n",
      "0.043478260869565216 \t0.03125 \t0.36168253\n",
      "0.045454545454545456 \t0.03125 \t0.36315808\n",
      "0.047619047619047616 \t0.03125 \t0.36364228\n",
      "0.05 \t0.03125 \t0.3642785\n",
      "0.05263157894736842 \t0.03125 \t0.36457166\n",
      "0.05555555555555555 \t0.03125 \t0.36459196\n",
      "0.058823529411764705 \t0.03125 \t0.36492455\n",
      "0.0625 \t0.03125 \t0.36520436\n",
      "0.06666666666666667 \t0.03125 \t0.36780778\n",
      "0.07142857142857142 \t0.03125 \t0.36805737\n",
      "0.07692307692307693 \t0.03125 \t0.36882403\n",
      "0.0 \t0.0 \t0.36899686\n",
      "0.0 \t0.0 \t0.37110332\n",
      "0.0 \t0.0 \t0.3726061\n",
      "0.0 \t0.0 \t0.3726263\n",
      "0.0 \t0.0 \t0.37303308\n",
      "0.0 \t0.0 \t0.37322325\n",
      "0.0 \t0.0 \t0.3801958\n",
      "0.0 \t0.0 \t0.38176054\n",
      "0.0 \t0.0 \t0.38217795\n",
      "0.0 \t0.0 \t0.3824019\n",
      "0.0 \t0.0 \t0.38489577\n",
      "0.0 \t0.0 \t0.3874187\n",
      "1.0 \t0.0 \t"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 843 is out of bounds for axis 0 with size 843",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b4cc3a85b819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 843 is out of bounds for axis 0 with size 843"
     ]
    }
   ],
   "source": [
    "# Load data from the /host/hybridvs_data/videos/frames_50X50_jackson-town-square\n",
    "X = np.zeros(tuple([1000] + list(resol) + [3]), dtype='float32')\n",
    "Y = [[0,1] for j in range(1000)]\n",
    "for i in range(0, 1000):\n",
    "    img_names = os.path.join('/host/hybridvs_data/videos/frames_50X50_jackson-town-square', str(i + 1).zfill(7) + '.jpg')\n",
    "    frame = cv2.imread(img_names)\n",
    "    frame = cv2.resize(frame, resol, interpolation=cv2.INTER_NEAREST)\n",
    "    X[i,:] = frame\n",
    "    if i in csv_ret:\n",
    "        Y[i] = [0,1]\n",
    "    else:\n",
    "        Y[i] = [1,0]\n",
    "        \n",
    "    #print frame\n",
    "    \n",
    "X /= 255.0\n",
    "\n",
    "probs = model.predict(X, batch_size=256, verbose=0)\n",
    "Y = np.array(Y)\n",
    "#print Y[:,1]\n",
    "#print probs[:,1]\n",
    "precision, recall, threshold = sklearn.metrics.precision_recall_curve(Y[:,1], probs[:,1])\n",
    "for i in range(0,len(precision)):\n",
    "    print precision[i], '\\t', recall[i], '\\t', threshold[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
